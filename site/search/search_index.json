{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Procesamiento del Lenguaje Natural (PLN) \u00b6 Introducci\u00f3n \u00b6 El Procesamiento del Lenguaje Natural (Natural Languaje Processing, NLP) es un \u00e1rea de la Inteligencia Artificial (IA) relacionada con el procesamiento, an\u00e1lisis y comprensi\u00f3n de cualquier lenguaje natural con el objetivo de crear sistemas basados en IA que sean capaces de interactuar con seres humanos en el mismo lenguaje de manera escrita o hablada. El PLN se encuentra en la zona intermedia entre la ling\u00fc\u00edstica y la IA. Un sistema de PLN puede actuar en tres escenarios diferentes: Sistemas en los que el texto es la entrada (input) del sistema. Sistemas en los que el texto es la salida (output) del sistema. Sistemas en los que el texto es tanto la entrada como la salida. \u00c1reas de aplicaci\u00f3n del PLN \u00b6 Las \u00e1reas del PLN son: NLP: Procesamiento del Lenguaje Natural. NLU: Entendimiento del Lenguaje Natural. NLG: Generaci\u00f3n del Lenguaje Natural. Aplicaciones del PLN \u00b6 Recuperaci\u00f3n de informaci\u00f3n (Information Retrieval) Extracci\u00f3n de informaci\u00f3n (Information Extraction) QA, Respuestas a preguntas (Question Answering) Clasificaci\u00f3n textual (Text Clasification) NLG, generaci\u00f3n de lenguaje natural (Natural Languaje Generation) Conceptos importantes \u00b6 Unidades de comunicaci\u00f3n b\u00e1sica \u00b6 Desde el punto de vista computacional, en vez de trabajar con palabras, se trabaja con el concepto de token . Un token es una secuencia de caracteres separados por espacio en blanco. La tokenizaci\u00f3n sirve como base para varias tareas de NLP, como la clasificaci\u00f3n de texto, el an\u00e1lisis de sentimientos y el reconocimiento de entidades con nombre. Consiste en dividir el texto unidades m\u00e1s peque\u00f1as llamadas tokens. Todos los tokens de un texto que sean iguales se considera que pertenecen al mismo type o tipo. Si en un texto aparece, por ejemplo, cinco veces la palabra amigo, se dice que son cinco tokens del type amigo. Ejemplo: Es amigo de mi amigo Types : es amigo de mi (cuatro types) Tokens : es amigo de mi amigo (cinco tokens) El lema es la forma de nombrar una palabra y todas sus derivaciones morfol\u00f3gicas. En el caso de los verbos, el lema suele ser la forma de infinitivo y en el caso de los nombres, la forma de masculino singular. Si por ejemplo aparece en un texto types como \"cantar\u00eda\", \"cant\u00e1bamos\", \"cantar\u00e9\", etc. Se dice que todos sus tokens pertenecen al lema \"cantar\". A este proceso se le llama lematizaci\u00f3n . El stemming consiste en reducir cada token a su raiz o lexema: la parte invariable que asume el significado de la palabra. Si tengo la palabra caminando, elimian \"ando\" y deja \"camin\". Vectores \u00b6 Los vectores son representaciones num\u00e9ricas de texto, es decir es la forma en que traducimos el texto a un formato que un modelo de aprendizaje autom\u00e1tico puede entender y aprender. El objetivo es tener representaciones vectoriales \u00fatiles. Proyector de embeddings Stop Words \u00b6 Son palabras que no dan mucho significado a la oraci\u00f3n (y, el, es, ...). Aparecen muchas veces en todas las oraciones pero no aportan mucha informaci\u00f3n. Si ignoramos estas stop words, los vectores van a tener menos dimensiones y se va a poder trabajar mejor. Corpus \u00b6 Un corpus ling\u00fc\u00edstico es un conjunto de textos relativamente grande, que refleja una lengua. Es decir, es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Encontrar un buen corpus sobre el cual trabajar no suele ser una tarea sencilla; uno que se suele utilizar para entrenar modelos es la informaci\u00f3n de wikipedia. Bolsa de palabras \u00b6 Bag of Words (BoW) es una t\u00e9cnica de PLN que busca representar documentos como vectores num\u00e9ricos. Es un modelo de representaci\u00f3n que se basa en la definici\u00f3n de un vector de longitud constante donde se reflejan las ocurrencias de cada uno de los t\u00e9rminos existentes a lo largo de las unidades que forma parte del corpus (p\u00e1rrafo, documento, conjunto de documentos). Es una t\u00e9cnica que surgi\u00f3 en el a\u00f1o 1954. Describe toda la informaci\u00f3n que hay en cada una de las frases. Se representa mediante una tabla donde se marca el n\u00famero de veces que una palabra aparece en cada uno de los documentos. Importante No considera la estructura gramatical ni el orden de las palabras. Esta t\u00e9cnica no considera el orden de las palabras aunque es bastante importante. Por ejemplo, no significar\u00eda lo mismo \"la casa de Mar\u00eda \" que \"la Mar\u00eda de casa\". Se puede utilizar la bolsa de trabajo por ejemplo, para el an\u00e1lisis de sentimientos, donde nos interesar\u00eda saber si el sentimiento es positivo o negativo. Tambi\u00e9n se puede utilizar en la detecci\u00f3n de spam, ya que solamente tendr\u00eda que buscar parlabras del tipo: premio, ganador, etc. para saber si ese correo es spam o no. En el caso de CHATGPT, no podr\u00edamos utilizarlo ya que s\u00ed que es importante el orden que tienen las palabras. Una de las t\u00e9cnicas que se utilizan en la bolsa de palabras para convertir el texto en vector es el conteo de palabras que consisten en lo siguiente: Determinar el tama\u00f1o del vocabulario, es contar las palabras \u00fanicas que aparecen en los documentos. Creamos un vector basado en el tama\u00f1o del vocabulario. Cada posici\u00f3n representa una palabra en el vocabulario. Eliminaci\u00f3n de las palabras m\u00e1s frecuentes. Se eliminan las stopwords as\u00ed como palabras comunes a todos los documentos. Con esto, ya podemos sacar una primera conclusi\u00f3n, el documento uno es un sentimiento positivo de las pel\u00edculas. El documento dos un sentimiento negativo y el tres negativo. Construcci\u00f3n de un BoW: Construcci\u00f3n del vocabulario. Se tiene que preprocesar el documento con anterioridad. Vectorizar los documentos. Crear un vector donde cada posici\u00f3n representa una palabra en el vocabulario. Eliminamos las palabras m\u00e1s frecuentes. No solo las stopwords sino tambi\u00e9n palabras que se repiten mucho y no aportan informaci\u00f3n. Vectorizaci\u00f3n de documentos con el documento totalmente preprocesado. Normalizaci\u00f3n de las frecuencias. Si comparamos documentos de diferentes tama\u00f1os, tenemos que dividir la frecuencia de cada palabra entre la longitud total del documento en el que se encuentra. Cuando ya tenemos construido el BOW, tendremos una matriz donde cada fila representa un documento y cada columna una palabra del vocabulario y los valores de la matriz, son la frecuencia de dicha palabra en dicho documento. Modelos n-gramas \u00b6 Los modelos n-gramas son modelos estad\u00edsticos para predecir la siguiente palabra dada una secuencia de elementos ling\u00fc\u00edsticos. Un n-grama es una secuencia de n elementos como pueden ser palabras, letras, etc. Ejemplo: \"La vida es bella\" 1-grama: \"la\",\"vida\",\"es\",\"bella\" 2-grama: \"la vida\", \"vida es\", \"es bella\" 3-grama: \"la vida es\",\"vida es bella\" ... Cuantos m\u00e1s n-gramas tengamos, m\u00e1s informaci\u00f3n tendremos sobre el contexto. Los modelos de n-gramas funcionan de la siguiente manera: Los modelos se construyen calculando la frecuencia de cada n-grama en el conjunto de datos. Bas\u00e1ndonos en dichas frecuencias, se calculan las probabilidades. Por ejemplo si digo \"la vida es \", habr\u00e1 m\u00e1s probabilidades de que a continuaci\u00f3n vaya la palabra \"bella\" que la palabra \"fea\". Estos modelos se utilizan en la predicci\u00f3n de texto, generaci\u00f3n de texto, transcripci\u00f3n de voz a texto, clasificaci\u00f3n de texto. Actividad 1.1 \u00b6 Actividad 1.1 Crea un notebook dividido en dos partes: BoW Descarga un libro. Realiza el preprocesamiento. Justifica el tema del libro. N-grama Realiza la pr\u00e1ctica anterior con 2-grama y 4-grama. Conclusiones.","title":"1. Procesamiento del Lenguaje Natural"},{"location":"index.html#procesamiento-del-lenguaje-natural-pln","text":"","title":"Procesamiento del Lenguaje Natural (PLN)"},{"location":"index.html#introduccion","text":"El Procesamiento del Lenguaje Natural (Natural Languaje Processing, NLP) es un \u00e1rea de la Inteligencia Artificial (IA) relacionada con el procesamiento, an\u00e1lisis y comprensi\u00f3n de cualquier lenguaje natural con el objetivo de crear sistemas basados en IA que sean capaces de interactuar con seres humanos en el mismo lenguaje de manera escrita o hablada. El PLN se encuentra en la zona intermedia entre la ling\u00fc\u00edstica y la IA. Un sistema de PLN puede actuar en tres escenarios diferentes: Sistemas en los que el texto es la entrada (input) del sistema. Sistemas en los que el texto es la salida (output) del sistema. Sistemas en los que el texto es tanto la entrada como la salida.","title":"Introducci\u00f3n"},{"location":"index.html#areas-de-aplicacion-del-pln","text":"Las \u00e1reas del PLN son: NLP: Procesamiento del Lenguaje Natural. NLU: Entendimiento del Lenguaje Natural. NLG: Generaci\u00f3n del Lenguaje Natural.","title":"\u00c1reas de aplicaci\u00f3n del PLN"},{"location":"index.html#aplicaciones-del-pln","text":"Recuperaci\u00f3n de informaci\u00f3n (Information Retrieval) Extracci\u00f3n de informaci\u00f3n (Information Extraction) QA, Respuestas a preguntas (Question Answering) Clasificaci\u00f3n textual (Text Clasification) NLG, generaci\u00f3n de lenguaje natural (Natural Languaje Generation)","title":"Aplicaciones del PLN"},{"location":"index.html#conceptos-importantes","text":"","title":"Conceptos importantes"},{"location":"index.html#unidades-de-comunicacion-basica","text":"Desde el punto de vista computacional, en vez de trabajar con palabras, se trabaja con el concepto de token . Un token es una secuencia de caracteres separados por espacio en blanco. La tokenizaci\u00f3n sirve como base para varias tareas de NLP, como la clasificaci\u00f3n de texto, el an\u00e1lisis de sentimientos y el reconocimiento de entidades con nombre. Consiste en dividir el texto unidades m\u00e1s peque\u00f1as llamadas tokens. Todos los tokens de un texto que sean iguales se considera que pertenecen al mismo type o tipo. Si en un texto aparece, por ejemplo, cinco veces la palabra amigo, se dice que son cinco tokens del type amigo. Ejemplo: Es amigo de mi amigo Types : es amigo de mi (cuatro types) Tokens : es amigo de mi amigo (cinco tokens) El lema es la forma de nombrar una palabra y todas sus derivaciones morfol\u00f3gicas. En el caso de los verbos, el lema suele ser la forma de infinitivo y en el caso de los nombres, la forma de masculino singular. Si por ejemplo aparece en un texto types como \"cantar\u00eda\", \"cant\u00e1bamos\", \"cantar\u00e9\", etc. Se dice que todos sus tokens pertenecen al lema \"cantar\". A este proceso se le llama lematizaci\u00f3n . El stemming consiste en reducir cada token a su raiz o lexema: la parte invariable que asume el significado de la palabra. Si tengo la palabra caminando, elimian \"ando\" y deja \"camin\".","title":"Unidades de comunicaci\u00f3n b\u00e1sica"},{"location":"index.html#vectores","text":"Los vectores son representaciones num\u00e9ricas de texto, es decir es la forma en que traducimos el texto a un formato que un modelo de aprendizaje autom\u00e1tico puede entender y aprender. El objetivo es tener representaciones vectoriales \u00fatiles. Proyector de embeddings","title":"Vectores"},{"location":"index.html#stop-words","text":"Son palabras que no dan mucho significado a la oraci\u00f3n (y, el, es, ...). Aparecen muchas veces en todas las oraciones pero no aportan mucha informaci\u00f3n. Si ignoramos estas stop words, los vectores van a tener menos dimensiones y se va a poder trabajar mejor.","title":"Stop Words"},{"location":"index.html#corpus","text":"Un corpus ling\u00fc\u00edstico es un conjunto de textos relativamente grande, que refleja una lengua. Es decir, es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Encontrar un buen corpus sobre el cual trabajar no suele ser una tarea sencilla; uno que se suele utilizar para entrenar modelos es la informaci\u00f3n de wikipedia.","title":"Corpus"},{"location":"index.html#bolsa-de-palabras","text":"Bag of Words (BoW) es una t\u00e9cnica de PLN que busca representar documentos como vectores num\u00e9ricos. Es un modelo de representaci\u00f3n que se basa en la definici\u00f3n de un vector de longitud constante donde se reflejan las ocurrencias de cada uno de los t\u00e9rminos existentes a lo largo de las unidades que forma parte del corpus (p\u00e1rrafo, documento, conjunto de documentos). Es una t\u00e9cnica que surgi\u00f3 en el a\u00f1o 1954. Describe toda la informaci\u00f3n que hay en cada una de las frases. Se representa mediante una tabla donde se marca el n\u00famero de veces que una palabra aparece en cada uno de los documentos. Importante No considera la estructura gramatical ni el orden de las palabras. Esta t\u00e9cnica no considera el orden de las palabras aunque es bastante importante. Por ejemplo, no significar\u00eda lo mismo \"la casa de Mar\u00eda \" que \"la Mar\u00eda de casa\". Se puede utilizar la bolsa de trabajo por ejemplo, para el an\u00e1lisis de sentimientos, donde nos interesar\u00eda saber si el sentimiento es positivo o negativo. Tambi\u00e9n se puede utilizar en la detecci\u00f3n de spam, ya que solamente tendr\u00eda que buscar parlabras del tipo: premio, ganador, etc. para saber si ese correo es spam o no. En el caso de CHATGPT, no podr\u00edamos utilizarlo ya que s\u00ed que es importante el orden que tienen las palabras. Una de las t\u00e9cnicas que se utilizan en la bolsa de palabras para convertir el texto en vector es el conteo de palabras que consisten en lo siguiente: Determinar el tama\u00f1o del vocabulario, es contar las palabras \u00fanicas que aparecen en los documentos. Creamos un vector basado en el tama\u00f1o del vocabulario. Cada posici\u00f3n representa una palabra en el vocabulario. Eliminaci\u00f3n de las palabras m\u00e1s frecuentes. Se eliminan las stopwords as\u00ed como palabras comunes a todos los documentos. Con esto, ya podemos sacar una primera conclusi\u00f3n, el documento uno es un sentimiento positivo de las pel\u00edculas. El documento dos un sentimiento negativo y el tres negativo. Construcci\u00f3n de un BoW: Construcci\u00f3n del vocabulario. Se tiene que preprocesar el documento con anterioridad. Vectorizar los documentos. Crear un vector donde cada posici\u00f3n representa una palabra en el vocabulario. Eliminamos las palabras m\u00e1s frecuentes. No solo las stopwords sino tambi\u00e9n palabras que se repiten mucho y no aportan informaci\u00f3n. Vectorizaci\u00f3n de documentos con el documento totalmente preprocesado. Normalizaci\u00f3n de las frecuencias. Si comparamos documentos de diferentes tama\u00f1os, tenemos que dividir la frecuencia de cada palabra entre la longitud total del documento en el que se encuentra. Cuando ya tenemos construido el BOW, tendremos una matriz donde cada fila representa un documento y cada columna una palabra del vocabulario y los valores de la matriz, son la frecuencia de dicha palabra en dicho documento.","title":"Bolsa de palabras"},{"location":"index.html#modelos-n-gramas","text":"Los modelos n-gramas son modelos estad\u00edsticos para predecir la siguiente palabra dada una secuencia de elementos ling\u00fc\u00edsticos. Un n-grama es una secuencia de n elementos como pueden ser palabras, letras, etc. Ejemplo: \"La vida es bella\" 1-grama: \"la\",\"vida\",\"es\",\"bella\" 2-grama: \"la vida\", \"vida es\", \"es bella\" 3-grama: \"la vida es\",\"vida es bella\" ... Cuantos m\u00e1s n-gramas tengamos, m\u00e1s informaci\u00f3n tendremos sobre el contexto. Los modelos de n-gramas funcionan de la siguiente manera: Los modelos se construyen calculando la frecuencia de cada n-grama en el conjunto de datos. Bas\u00e1ndonos en dichas frecuencias, se calculan las probabilidades. Por ejemplo si digo \"la vida es \", habr\u00e1 m\u00e1s probabilidades de que a continuaci\u00f3n vaya la palabra \"bella\" que la palabra \"fea\". Estos modelos se utilizan en la predicci\u00f3n de texto, generaci\u00f3n de texto, transcripci\u00f3n de voz a texto, clasificaci\u00f3n de texto.","title":"Modelos n-gramas"},{"location":"index.html#actividad-11","text":"Actividad 1.1 Crea un notebook dividido en dos partes: BoW Descarga un libro. Realiza el preprocesamiento. Justifica el tema del libro. N-grama Realiza la pr\u00e1ctica anterior con 2-grama y 4-grama. Conclusiones.","title":"Actividad 1.1"},{"location":"p.html","text":"EMBEDDING \u00b6 Un embedding es una representaci\u00f3n num\u00e9rica de las palabras en un espacio vectorial. Cada dimensi\u00f3n del vector, recoge aspectos sem\u00e1nticos o sint\u00e1cticos de cada palabra. La finalidad de los embeddings es que las m\u00e1quinas comprendan el significado de las palabras de manera efectiva. Son principalmente \u00fatiles para tareas como el an\u00e1lisis de sentimientos o la traducci\u00f3n autom\u00e1tica. Caracter\u00edsticas de los embeddings \u00b6 Captura sem\u00e1ntica y sint\u00e1ctica. Aprende el significado de la palabra y las relaciones sint\u00e1cticas que tiene. Permite la transmisi\u00f3n de conocimiento, es decir se puede entrenar en wikipedia y usarse para cualquier tarea. Contextualidad. Permite variar el significado de la palabra seg\u00fan el contexto. Ejemplo: banco. Resuelve analog\u00edas. Por ejemplo: Madrid es a Espa\u00f1a como Par\u00eds es a ... Las palabras pertenecientes a un mismo campo sem\u00e1ntico se encuentran pr\u00f3ximas entre ellas. Crear embeddings \u00b6 Partimos de un conjunto de documentos sobre los que vamos a aprender. Ejemplo: Wikipedia Utilizamos modelos de IA para comprender el significado de las palabras. En principio utilizamos n\u00fameros aleatorios para crear los vectores. A medida que el modelo recorre distintos textos, va ajustando estos n\u00fameros para que reflejen las relaciones que existen en el texto. Problemas para entrenar embeddings \u00b6 Requiere cantidad grandes de datos (wikipedia, libros, etc). Necesita tecnolog\u00edas costosas como GPU's. Utilizan mucho tiempo en el entrenamiento (dias, semanas,...). El ajuste de par\u00e1metros requiere m\u00faltiples iteraciones que conlleva costes en tiempo y recursos. Embeddings entrenados \u00b6 Vistos los problemas que puede generar entrenar los embeddings desde cero, la mejor soluci\u00f3n es utilizar embeddings que ya est\u00e9n entrenados para realizar nuevas tareas. Los m\u00e1s utilizados son BERT, MUSE y GPT. BERT \u00b6 BERT (Representaciones de Codificadores Bidireccional de Transformadores) fue desarrollado por Google en 2018. Se entren\u00f3 sobre un corpus compuesto por noticias, libros y art\u00edculos de distinto \u00e1mbito. Utiliza el \"mecanismo de atenci\u00f3n\" que permite entender las palabras y las relaciones entre ellas seg\u00fan el contexto. MUSE \u00b6 MUSE (Embeddings supervisados y no supervisados multiling\u00fces) es un embedding en el que se comparten palabras de distintos idiomas. Creado por Facebook en 2018. GPT \u00b6 Los modelos GPT han supuesto una verdadera revoluci\u00f3n. GPT (Generativos, Preentrenados, Transformers). La \u00faltima versi\u00f3n que tenemos es GPT4 que ha sido entrenado con un conjunto de datos muy grande. GPT es capaz de detectar mayores matices del lenguaje, adem\u00e1s de interactuar con interfaces externas para realizar tareas m\u00e1s complejas.","title":"3. Embedding"},{"location":"p.html#embedding","text":"Un embedding es una representaci\u00f3n num\u00e9rica de las palabras en un espacio vectorial. Cada dimensi\u00f3n del vector, recoge aspectos sem\u00e1nticos o sint\u00e1cticos de cada palabra. La finalidad de los embeddings es que las m\u00e1quinas comprendan el significado de las palabras de manera efectiva. Son principalmente \u00fatiles para tareas como el an\u00e1lisis de sentimientos o la traducci\u00f3n autom\u00e1tica.","title":"EMBEDDING"},{"location":"p.html#caracteristicas-de-los-embeddings","text":"Captura sem\u00e1ntica y sint\u00e1ctica. Aprende el significado de la palabra y las relaciones sint\u00e1cticas que tiene. Permite la transmisi\u00f3n de conocimiento, es decir se puede entrenar en wikipedia y usarse para cualquier tarea. Contextualidad. Permite variar el significado de la palabra seg\u00fan el contexto. Ejemplo: banco. Resuelve analog\u00edas. Por ejemplo: Madrid es a Espa\u00f1a como Par\u00eds es a ... Las palabras pertenecientes a un mismo campo sem\u00e1ntico se encuentran pr\u00f3ximas entre ellas.","title":"Caracter\u00edsticas de los embeddings"},{"location":"p.html#crear-embeddings","text":"Partimos de un conjunto de documentos sobre los que vamos a aprender. Ejemplo: Wikipedia Utilizamos modelos de IA para comprender el significado de las palabras. En principio utilizamos n\u00fameros aleatorios para crear los vectores. A medida que el modelo recorre distintos textos, va ajustando estos n\u00fameros para que reflejen las relaciones que existen en el texto.","title":"Crear embeddings"},{"location":"p.html#problemas-para-entrenar-embeddings","text":"Requiere cantidad grandes de datos (wikipedia, libros, etc). Necesita tecnolog\u00edas costosas como GPU's. Utilizan mucho tiempo en el entrenamiento (dias, semanas,...). El ajuste de par\u00e1metros requiere m\u00faltiples iteraciones que conlleva costes en tiempo y recursos.","title":"Problemas para entrenar embeddings"},{"location":"p.html#embeddings-entrenados","text":"Vistos los problemas que puede generar entrenar los embeddings desde cero, la mejor soluci\u00f3n es utilizar embeddings que ya est\u00e9n entrenados para realizar nuevas tareas. Los m\u00e1s utilizados son BERT, MUSE y GPT.","title":"Embeddings entrenados"},{"location":"p.html#bert","text":"BERT (Representaciones de Codificadores Bidireccional de Transformadores) fue desarrollado por Google en 2018. Se entren\u00f3 sobre un corpus compuesto por noticias, libros y art\u00edculos de distinto \u00e1mbito. Utiliza el \"mecanismo de atenci\u00f3n\" que permite entender las palabras y las relaciones entre ellas seg\u00fan el contexto.","title":"BERT"},{"location":"p.html#muse","text":"MUSE (Embeddings supervisados y no supervisados multiling\u00fces) es un embedding en el que se comparten palabras de distintos idiomas. Creado por Facebook en 2018.","title":"MUSE"},{"location":"p.html#gpt","text":"Los modelos GPT han supuesto una verdadera revoluci\u00f3n. GPT (Generativos, Preentrenados, Transformers). La \u00faltima versi\u00f3n que tenemos es GPT4 que ha sido entrenado con un conjunto de datos muy grande. GPT es capaz de detectar mayores matices del lenguaje, adem\u00e1s de interactuar con interfaces externas para realizar tareas m\u00e1s complejas.","title":"GPT"},{"location":"p1.html","text":"Librer\u00edas para PLN \u00b6 NLTK: Natural Language Toolkit \u00b6 Es una librer\u00eda de Python utilizada en el procesamiento del lenguaje natural. NLTK ofrece potentes capacidades de tokenizaci\u00f3n que facilitan el procesamiento eficiente de datos textuales. La tokenizaci\u00f3n de palabras de NLTK le permite dividir el texto en palabras individuales o tokens, adem\u00e1s contiene corpus de textos y vocabulario organizado como WordNet . M\u00e9todos en NLTK \u00b6 Nltk.chunk: Utiliza expresiones regulares o \u00e1rboles de an\u00e1lisis para extraer frases. Nltk.tokenize: Divide el texto en frases, palabras o subpalabras, esencial para la estructura del texto. Nltk.corpus: Colecci\u00f3n de textos que se utiliza como muestra del lenguaje real. Nltk.sentiment: Analiza el sentimiento de un texto, como positivo o negativo. Nltk.translate: Funciones y algoritmos para trabajar con traducci\u00f3n autom\u00e1tica. Nltk.stem: Reduce las palabras a su ra\u00edz, eliminando sufijos y prefijos. Nltk.tag: Asigna partes del discurso a palabras como sustantivos, verbos, adjetivos, ... Nltk.inference: Herramienta para trabajar con l\u00f3gica e inferencia en textos. Tokenizaci\u00f3n con NLTK \u00b6 Importamos la librer\u00eda nltk y descargamos punkt que es el tokenizador de NLTK, el algoritmo que lleva a cabo la tokenizaci\u00f3n de las palabras que pases por par\u00e1metro. Divide el texto en frases a trav\u00e9s de un modelo no supervisado, esto es que no necesita datos etiquetados para su entrenamiento, obtendr\u00e1 patrones de los propios datos. import nltk nltk . download ( 'punkt' ) from nltk.corpus import stopwords from nltk.tokenize import sent_tokenize , word_tokenize Dado un texto, podemos ver los tokens con el siguiente c\u00f3digo: tokens = nltk . word_tokenize ( texto ) Descargamos las stopwords nltk . download ( 'stopwords' ) Seleccionamos \u00fanicamente las stopwords en Espa\u00f1ol stop_words = set ( stopwords . words ( 'spanish' )) Tokenizaci\u00f3n tokens = word_tokenize ( texto ) Sacamos \u00fanicamente las palabras que no est\u00e1n en las stop_words texto_filtrado = [ word for word in tokens if not word in stop_words ] \u00bfQu\u00e9 sucede si no tenemos alguna stopword en may\u00fasculas? No las reconoce como stopwords por lo tanto lo primero que tenemos que hacer es pasar todo el texto a min\u00fasculas. texto = texto . lower () Stemming con NLTK \u00b6 nltk . download ( 'wordnet' ) from nltk.stem import SnowballStemmer # Creamos el stemmer en espa\u00f1ol stemmer = SnowballStemmer ( 'spanish' ) Probamos con la palabra caminando print ( stemmer . stem ( 'caminando' )) Lematizaci\u00f3n con NLTK \u00b6 \u00danicamente tiene el diccionario en Ingl\u00e9s. from nltk.stem import WordNetLemmatizer nltk . download ( 'wordnet' ) Inicializamos el tokenizador lematizador = WordNetLemmatizer () print ( lematizador . lemmatize ( \"running\" , \"v\" )) Spacy \u00b6 Es otra librer\u00eda de Python para PLN muy similar a NLTK y su implementaci\u00f3n suele ser m\u00e1s r\u00e1pida y precisa. M\u00e9todos en Spacy \u00b6 Spacy.load: Carga un modelo de lenguaje espec\u00edfico. Ejm: nlp=spacy.load('es_core_web_sm). nlp: Procesa un texto y ejecuta la cadena de procesamiento del modelo sobre \u00e9l. Doc, Span, Token: Proporciona contenedores para acceder a informaci\u00f3n sobre el texto como palabras individuales o grupos de palabras. Token.lemma_: Obtiene la forma base de una palabra. Doc.sents: Divide un documento en frases. Doc.ents: Extrae entidades nombradas como personas, organizaciones o lugares. Lematizaci\u00f3n en Spacy \u00b6 Instalamos Spacy ! pip install spacy - q Descargamos los paquetes que necesitamos para la lematizaci\u00f3n. ! python - m spacy download es_core_news_sm - q Cargar el modelo en espa\u00f1ol import spacy nlp = spacy . load ( 'es_core_news_sm' ) Probamos con texto (cualquier frase) doc = nlp ( 'texto' ) Imprimimos el texto y el lema de cada token for token in doc : print ( token . text , '->' , token . lemma_ ) Podemos mostrar cada token que etiqueta tiene for token in doc : print ( token . text , '->' , token . pos_ ) Actividad 2.1 \u00b6 Actividad 2.1 Crea un notebook dividido en dos partes: NLTK Realiza la tokenizaci\u00f3n del texto: \"El gato y el perro salen a jugar a la calle.\" Explica cada paso y visualiza los tokens finales. Comprobar el stemming de diferentes tiempos del verbo pasear. Explica el resultado. Haz la lematizaci\u00f3n de 5 palabras en ingl\u00e9s y explica el resultado. Spacy Realiza la lematizaci\u00f3n de un texto en castellano y explica los resultados. Aplica alguno de los m\u00e9todos de Spacy.","title":"2. Librer\u00edas para PLN"},{"location":"p1.html#librerias-para-pln","text":"","title":"Librer\u00edas para PLN"},{"location":"p1.html#nltk-natural-language-toolkit","text":"Es una librer\u00eda de Python utilizada en el procesamiento del lenguaje natural. NLTK ofrece potentes capacidades de tokenizaci\u00f3n que facilitan el procesamiento eficiente de datos textuales. La tokenizaci\u00f3n de palabras de NLTK le permite dividir el texto en palabras individuales o tokens, adem\u00e1s contiene corpus de textos y vocabulario organizado como WordNet .","title":"NLTK: Natural Language Toolkit"},{"location":"p1.html#metodos-en-nltk","text":"Nltk.chunk: Utiliza expresiones regulares o \u00e1rboles de an\u00e1lisis para extraer frases. Nltk.tokenize: Divide el texto en frases, palabras o subpalabras, esencial para la estructura del texto. Nltk.corpus: Colecci\u00f3n de textos que se utiliza como muestra del lenguaje real. Nltk.sentiment: Analiza el sentimiento de un texto, como positivo o negativo. Nltk.translate: Funciones y algoritmos para trabajar con traducci\u00f3n autom\u00e1tica. Nltk.stem: Reduce las palabras a su ra\u00edz, eliminando sufijos y prefijos. Nltk.tag: Asigna partes del discurso a palabras como sustantivos, verbos, adjetivos, ... Nltk.inference: Herramienta para trabajar con l\u00f3gica e inferencia en textos.","title":"M\u00e9todos en NLTK"},{"location":"p1.html#tokenizacion-con-nltk","text":"Importamos la librer\u00eda nltk y descargamos punkt que es el tokenizador de NLTK, el algoritmo que lleva a cabo la tokenizaci\u00f3n de las palabras que pases por par\u00e1metro. Divide el texto en frases a trav\u00e9s de un modelo no supervisado, esto es que no necesita datos etiquetados para su entrenamiento, obtendr\u00e1 patrones de los propios datos. import nltk nltk . download ( 'punkt' ) from nltk.corpus import stopwords from nltk.tokenize import sent_tokenize , word_tokenize Dado un texto, podemos ver los tokens con el siguiente c\u00f3digo: tokens = nltk . word_tokenize ( texto ) Descargamos las stopwords nltk . download ( 'stopwords' ) Seleccionamos \u00fanicamente las stopwords en Espa\u00f1ol stop_words = set ( stopwords . words ( 'spanish' )) Tokenizaci\u00f3n tokens = word_tokenize ( texto ) Sacamos \u00fanicamente las palabras que no est\u00e1n en las stop_words texto_filtrado = [ word for word in tokens if not word in stop_words ] \u00bfQu\u00e9 sucede si no tenemos alguna stopword en may\u00fasculas? No las reconoce como stopwords por lo tanto lo primero que tenemos que hacer es pasar todo el texto a min\u00fasculas. texto = texto . lower ()","title":"Tokenizaci\u00f3n con NLTK"},{"location":"p1.html#stemming-con-nltk","text":"nltk . download ( 'wordnet' ) from nltk.stem import SnowballStemmer # Creamos el stemmer en espa\u00f1ol stemmer = SnowballStemmer ( 'spanish' ) Probamos con la palabra caminando print ( stemmer . stem ( 'caminando' ))","title":"Stemming con NLTK"},{"location":"p1.html#lematizacion-con-nltk","text":"\u00danicamente tiene el diccionario en Ingl\u00e9s. from nltk.stem import WordNetLemmatizer nltk . download ( 'wordnet' ) Inicializamos el tokenizador lematizador = WordNetLemmatizer () print ( lematizador . lemmatize ( \"running\" , \"v\" ))","title":"Lematizaci\u00f3n con NLTK"},{"location":"p1.html#spacy","text":"Es otra librer\u00eda de Python para PLN muy similar a NLTK y su implementaci\u00f3n suele ser m\u00e1s r\u00e1pida y precisa.","title":"Spacy"},{"location":"p1.html#metodos-en-spacy","text":"Spacy.load: Carga un modelo de lenguaje espec\u00edfico. Ejm: nlp=spacy.load('es_core_web_sm). nlp: Procesa un texto y ejecuta la cadena de procesamiento del modelo sobre \u00e9l. Doc, Span, Token: Proporciona contenedores para acceder a informaci\u00f3n sobre el texto como palabras individuales o grupos de palabras. Token.lemma_: Obtiene la forma base de una palabra. Doc.sents: Divide un documento en frases. Doc.ents: Extrae entidades nombradas como personas, organizaciones o lugares.","title":"M\u00e9todos en Spacy"},{"location":"p1.html#lematizacion-en-spacy","text":"Instalamos Spacy ! pip install spacy - q Descargamos los paquetes que necesitamos para la lematizaci\u00f3n. ! python - m spacy download es_core_news_sm - q Cargar el modelo en espa\u00f1ol import spacy nlp = spacy . load ( 'es_core_news_sm' ) Probamos con texto (cualquier frase) doc = nlp ( 'texto' ) Imprimimos el texto y el lema de cada token for token in doc : print ( token . text , '->' , token . lemma_ ) Podemos mostrar cada token que etiqueta tiene for token in doc : print ( token . text , '->' , token . pos_ )","title":"Lematizaci\u00f3n en Spacy"},{"location":"p1.html#actividad-21","text":"Actividad 2.1 Crea un notebook dividido en dos partes: NLTK Realiza la tokenizaci\u00f3n del texto: \"El gato y el perro salen a jugar a la calle.\" Explica cada paso y visualiza los tokens finales. Comprobar el stemming de diferentes tiempos del verbo pasear. Explica el resultado. Haz la lematizaci\u00f3n de 5 palabras en ingl\u00e9s y explica el resultado. Spacy Realiza la lematizaci\u00f3n de un texto en castellano y explica los resultados. Aplica alguno de los m\u00e9todos de Spacy.","title":"Actividad 2.1"},{"location":"p4.html","text":"An\u00e1lisis de Sentimiento \u00b6 En el an\u00e1lisis de sentimientos se puede interpretar, analizar y categorizar las opiniones y emociones humanas expresadas en forma de texto. Categorizaci\u00f3n: positivo, negativo, neutral. Podemos considerar el an\u00e1lisis de sentimiento como una tarea de clasificaci\u00f3n etiquetando las entradas como positivas y negativas por ejemplo. Otra posibilidad es tratar el an\u00e1lisis de sentimiento como una tarea de regresi\u00f3n , es decir predecir un valor continuo en vez de una categor\u00eda. El an\u00e1lisis de sentimiento tiene en cuenta dos variables: Polaridad: es un comentario positivo o negativo. Intensidad: diferentes grados de la polaridad. Aplicaciones del an\u00e1lisis de sentimiento \u00b6 Redes sociales: analiza los comentarios en tiempo real para ver reacciones de la gente o reputaci\u00f3n. Rese\u00f1as de productos o servicios. Encuestas en l\u00ednea: para comprender mejor las preferencias de los clientes. Decisiones de inversi\u00f3n. M\u00e9todos para el an\u00e1lisis de sentimiento \u00b6 Una manera b\u00e1sica y sencilla es la detecci\u00f3n de palabras clave como bueno, malo, p\u00e9simo,... pero no comprende el contexto. El m\u00e9todo con Aprendizaje autom\u00e1tico, basado en t\u00e9cnicas de IA, construye un modelo que aprende a trav\u00e9s de miles de ejemplos a distinguir entre un comentario positivo y otro negativo. Descargamos el m\u00f3dulo para rese\u00f1as en espa\u00f1ol ! pip install sentiment - analysis - spanish Cargamos el modelo en memoria y lo almacenamos en la variable model_sentiment from sentiment_analysis_spanish import sentiment_analysis model_sentiment = sentiment_analysis . SentimentAnalysisSpanish () Definimos una funci\u00f3n que tiene como entrada el texto y el modelo y nos devuelve el sentimiento. def sentimiento ( texto , sentiment = model_sentiment ): return sentiment . sentiment ( texto ) Aplicamos nuestro modelo sobre por ejemplo los t\u00edtulos de mi dataset titulos_sentimiento titulos_sentimiento [ \"prediccion_ia\" ] = titulos_sentimiento [ \"title\" ] . apply ( lambda texto : sentimiento ( texto )) Ajusto las predicciones para quitar los decimales y que me aparezcan \u00fanicamente dos valores def ajusto_predicciones_ia ( prediccion ): if prediccion < 0.5 : return 0 else : return 3","title":"4. An\u00e1lisis de sentimiento"},{"location":"p4.html#analisis-de-sentimiento","text":"En el an\u00e1lisis de sentimientos se puede interpretar, analizar y categorizar las opiniones y emociones humanas expresadas en forma de texto. Categorizaci\u00f3n: positivo, negativo, neutral. Podemos considerar el an\u00e1lisis de sentimiento como una tarea de clasificaci\u00f3n etiquetando las entradas como positivas y negativas por ejemplo. Otra posibilidad es tratar el an\u00e1lisis de sentimiento como una tarea de regresi\u00f3n , es decir predecir un valor continuo en vez de una categor\u00eda. El an\u00e1lisis de sentimiento tiene en cuenta dos variables: Polaridad: es un comentario positivo o negativo. Intensidad: diferentes grados de la polaridad.","title":"An\u00e1lisis de Sentimiento"},{"location":"p4.html#aplicaciones-del-analisis-de-sentimiento","text":"Redes sociales: analiza los comentarios en tiempo real para ver reacciones de la gente o reputaci\u00f3n. Rese\u00f1as de productos o servicios. Encuestas en l\u00ednea: para comprender mejor las preferencias de los clientes. Decisiones de inversi\u00f3n.","title":"Aplicaciones del an\u00e1lisis de sentimiento"},{"location":"p4.html#metodos-para-el-analisis-de-sentimiento","text":"Una manera b\u00e1sica y sencilla es la detecci\u00f3n de palabras clave como bueno, malo, p\u00e9simo,... pero no comprende el contexto. El m\u00e9todo con Aprendizaje autom\u00e1tico, basado en t\u00e9cnicas de IA, construye un modelo que aprende a trav\u00e9s de miles de ejemplos a distinguir entre un comentario positivo y otro negativo. Descargamos el m\u00f3dulo para rese\u00f1as en espa\u00f1ol ! pip install sentiment - analysis - spanish Cargamos el modelo en memoria y lo almacenamos en la variable model_sentiment from sentiment_analysis_spanish import sentiment_analysis model_sentiment = sentiment_analysis . SentimentAnalysisSpanish () Definimos una funci\u00f3n que tiene como entrada el texto y el modelo y nos devuelve el sentimiento. def sentimiento ( texto , sentiment = model_sentiment ): return sentiment . sentiment ( texto ) Aplicamos nuestro modelo sobre por ejemplo los t\u00edtulos de mi dataset titulos_sentimiento titulos_sentimiento [ \"prediccion_ia\" ] = titulos_sentimiento [ \"title\" ] . apply ( lambda texto : sentimiento ( texto )) Ajusto las predicciones para quitar los decimales y que me aparezcan \u00fanicamente dos valores def ajusto_predicciones_ia ( prediccion ): if prediccion < 0.5 : return 0 else : return 3","title":"M\u00e9todos para el an\u00e1lisis de sentimiento"},{"location":"p5.html","text":"PLN con Redes Neuronales \u00b6 Las redes neuronales son una estructura de neuronas interconectaddas entre s\u00ed que procesan datos de manera similar al cerebro humano. La composici\u00f3n de las redes neuronales es la siguiente: Capa de entrada: recibe los datos iniciales. Capas ocultas: Procesan la informaci\u00f3n en base a las salidas de la capa anterior y generan representaciones intermedias. Capa de salida: genera la predicci\u00f3n o resultado final. Requiere tantas neuronas como salidas necesitemos. Redes Neuronales Recurrentes (RNR) \u00b6 Las Redes Neuronales Recurrentes son un tipo de arquitectura dise\u00f1ada para manejar datos con formato de secuencia como textos, audios o series temporales. Estas redes permiten mantener una especie de \"memoria\" de las entradas anteriores mientras se procesan las entradas actuales. Una red recurrente es capaz de procesar varias palabras a la vez y por tanto entender mejor el contexto de cada palabra. Algunos tipos de Redes Neuronales Recurrentes: RNN est\u00e1ndar: Tambi\u00e9n conocida como RNN de bucle simple, es la forma m\u00e1s b\u00e1sica de RNN. Tiene conexiones de retroalimentaci\u00f3n que permiten que la salida de una capa se retroalimente como entrada en la siguiente iteraci\u00f3n de tiempo. LSTM (Long Short-Term Memory): Las LSTM son una variante de las RNN dise\u00f1ada para abordar el problema del desvanecimiento del gradiente y capturar dependencias a largo plazo de manera m\u00e1s efectiva. Introducen unidades de memoria que permiten a la red aprender qu\u00e9 informaci\u00f3n retener y qu\u00e9 olvidar a lo largo del tiempo. from tensorflow.keras.layers import LSTM red . add ( LSTM ( 128 )) GRU (Gated Recurrent Unit): Similar a las LSTM, las GRU son otra variante de las RNN que tambi\u00e9n abordan el problema del desvanecimiento del gradiente. Tienen una estructura m\u00e1s simple que las LSTM, con menos par\u00e1metros, lo que puede llevar a un entrenamiento m\u00e1s r\u00e1pido y eficiente en algunos casos. from tensorflow.keras.layers import GRU red . add ( GRU ( 128 ) # Importar las bibliotecas necesarias import numpy as np from tensorflow.keras.datasets import imdb from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , Embedding , SimpleRNN # Definir los par\u00e1metros max_features = 10000 # N\u00famero m\u00e1ximo de palabras a considerar como caracter\u00edsticas maxlen = 100 # Truncar secuencias despu\u00e9s de este n\u00famero de pasos de tiempo batch_size = 32 # Cargar los datos ( x_train , y_train ), ( x_test , y_test ) = imdb . load_data ( num_words = max_features ) # Preprocesar los datos x_train = pad_sequences ( x_train , maxlen = maxlen ) x_test = pad_sequences ( x_test , maxlen = maxlen ) # Construir el modelo model = Sequential () model . add ( Embedding ( max_features , 32 )) model . add ( SimpleRNN ( 32 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) # Compilar el modelo model . compile ( optimizer = 'rmsprop' , loss = 'binary_crossentropy' , metrics = [ 'acc' ]) # Entrenar el modelo history = model . fit ( x_train , y_train , epochs = 10 , batch_size = batch_size , validation_split = 0.2 ) # Evaluar el modelo test_loss , test_acc = model . evaluate ( x_test , y_test ) print ( 'Test accuracy:' , test_acc ) # Guardar el modelo model . save ( 'sentiment_model1.h5' ) # Probar con otro conjunto de datos # Cargar el modelo # Cargar el modelo previamente entrenado model = tf . keras . models . load_model ( '/content/rnn_sentiment_model.h5' ) # Reemplaza 'rnn_sentiment_model.h5' con el nombre de tu archivo de modelo # Imprimir las 10 primeras listas del conjunto de entrenamiento for i in range ( 10 ): print ( f \"Secuencia { i + 1 } : { x_train [ i ] } \" ) # Tus propias revisiones de pel\u00edculas reviews = [ \"This movie was fantastic\" , \"solid performances in this straightforward adaption\" , \"brilliant documentary\" , \"worst mistake of my life\" , ] # Tokeniza tus revisiones utilizando el mismo Tokenizer que usaste para el modelo tokenizer = Tokenizer ( num_words = 10000 ) # Aseg\u00farate de que coincida con el valor que usaste para entrenar el modelo tokenizer . fit_on_texts ( reviews ) sequences = tokenizer . texts_to_sequences ( reviews ) # Aseg\u00farate de que todas las secuencias tengan la misma longitud max_sequence_length = 100 # Aseg\u00farate de que coincida con el valor que usaste para entrenar el modelo sequences = pad_sequences ( sequences , maxlen = max_sequence_length ) # Realiza predicciones en tus propias revisiones predictions = model . predict ( sequences ) # Muestra las predicciones for i , prediction in enumerate ( predictions ): sentiment = \"positivo\" if prediction > 0.5 else \"negativo\" print ( f \"Sentimiento de tu revisi\u00f3n { i + 1 } : { sentiment } (Puntuaci\u00f3n: { prediction [ 0 ] : .4f } )\" ) Redes Neuronales Convolucionales \u00b6 La convoluci\u00f3n es una operaci\u00f3n matem\u00e1tica que se utiliza para extraer caracter\u00edsticas importantes de los datos. Se utiliza mucho en el tratamiento de im\u00e1genes. Las CNN est\u00e1n formadas por tres tipos principales de capas: Capa convolucional: Es la primera capa de una red convolucional. Realiza la mayor\u00eda de los c\u00e1lculos y se encarga de identificar caracter\u00edsticas locales. Capa de activaci\u00f3n: a\u00f1aden funciones no lineales para permitir la aproximaci\u00f3n de funciones m\u00e1s complejas. Capa de agrupaci\u00f3n (pooling): Reduce la dimensionalidad de los datos y extrae caracter\u00edsticas m\u00e1s complejas. Capa totalmente conectada: La \u00faltima capa que procesa la informaci\u00f3n y produce la salida final. A medida que los datos de la imagen avanzan a trav\u00e9s de estas capas, la CNN reconoce elementos o formas m\u00e1s grandes, hasta identificar el objeto deseado. En el PLN se utilizan estas redes para aprender caracter\u00edsticas locales y globales del texto. from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , Embedding , Conv1D , GlobalMaxPooling1D Construimos la red de forma secuencial red = Sequential () A\u00f1adimos la capa de embedding red . add ( Embedding ( tamano_vocabulario , 128 )) A\u00f1adimos una capa de convoluciones con 64 filtros convolucionales de tama\u00f1o 5. red . add ( Conv1D ( 64 , 5 , activation = 'relu' )) Reducimos la dimensionalidad con pooling red . add ( GlobalMaxPooling1D ()) A\u00f1adimos una capa densa de 32 neuronas para procesar las representaciones obtenidas. red . add ( Dense ( 32 )) Por \u00faltimo haremos que nuestra red termine en una \u00fanica neurona con una funci\u00f3n de activaci\u00f3n sigmoidal que har\u00e1 que los resultados est\u00e9n pr\u00f3ximos a los extremos, es decir, a 0 o a 1. Al final lo que queremos es que la red nos clasifique las rese\u00f1as como positivas o negativas red . add ( Dense ( 1 , activation = 'sigmoid' )) Le indicamos el optimizador que debe usar para reducir la p\u00e9rdida y el tipo de m\u00e9trica red . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) Entrenamos el modelo red . fit ( x_train , y_train , batch_size = 32 , epochs = 5 , validation_data = ( x_test , y_test )) Evaluamos el modelo loss , accuracy = red . evaluate ( x_test , y_test ) print ( f 'Tasa de acierto: { accuracy } ' )","title":"5. PLN con Redes Neuronales "},{"location":"p5.html#pln-con-redes-neuronales","text":"Las redes neuronales son una estructura de neuronas interconectaddas entre s\u00ed que procesan datos de manera similar al cerebro humano. La composici\u00f3n de las redes neuronales es la siguiente: Capa de entrada: recibe los datos iniciales. Capas ocultas: Procesan la informaci\u00f3n en base a las salidas de la capa anterior y generan representaciones intermedias. Capa de salida: genera la predicci\u00f3n o resultado final. Requiere tantas neuronas como salidas necesitemos.","title":"PLN con Redes Neuronales"},{"location":"p5.html#redes-neuronales-recurrentes-rnr","text":"Las Redes Neuronales Recurrentes son un tipo de arquitectura dise\u00f1ada para manejar datos con formato de secuencia como textos, audios o series temporales. Estas redes permiten mantener una especie de \"memoria\" de las entradas anteriores mientras se procesan las entradas actuales. Una red recurrente es capaz de procesar varias palabras a la vez y por tanto entender mejor el contexto de cada palabra. Algunos tipos de Redes Neuronales Recurrentes: RNN est\u00e1ndar: Tambi\u00e9n conocida como RNN de bucle simple, es la forma m\u00e1s b\u00e1sica de RNN. Tiene conexiones de retroalimentaci\u00f3n que permiten que la salida de una capa se retroalimente como entrada en la siguiente iteraci\u00f3n de tiempo. LSTM (Long Short-Term Memory): Las LSTM son una variante de las RNN dise\u00f1ada para abordar el problema del desvanecimiento del gradiente y capturar dependencias a largo plazo de manera m\u00e1s efectiva. Introducen unidades de memoria que permiten a la red aprender qu\u00e9 informaci\u00f3n retener y qu\u00e9 olvidar a lo largo del tiempo. from tensorflow.keras.layers import LSTM red . add ( LSTM ( 128 )) GRU (Gated Recurrent Unit): Similar a las LSTM, las GRU son otra variante de las RNN que tambi\u00e9n abordan el problema del desvanecimiento del gradiente. Tienen una estructura m\u00e1s simple que las LSTM, con menos par\u00e1metros, lo que puede llevar a un entrenamiento m\u00e1s r\u00e1pido y eficiente en algunos casos. from tensorflow.keras.layers import GRU red . add ( GRU ( 128 ) # Importar las bibliotecas necesarias import numpy as np from tensorflow.keras.datasets import imdb from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , Embedding , SimpleRNN # Definir los par\u00e1metros max_features = 10000 # N\u00famero m\u00e1ximo de palabras a considerar como caracter\u00edsticas maxlen = 100 # Truncar secuencias despu\u00e9s de este n\u00famero de pasos de tiempo batch_size = 32 # Cargar los datos ( x_train , y_train ), ( x_test , y_test ) = imdb . load_data ( num_words = max_features ) # Preprocesar los datos x_train = pad_sequences ( x_train , maxlen = maxlen ) x_test = pad_sequences ( x_test , maxlen = maxlen ) # Construir el modelo model = Sequential () model . add ( Embedding ( max_features , 32 )) model . add ( SimpleRNN ( 32 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) # Compilar el modelo model . compile ( optimizer = 'rmsprop' , loss = 'binary_crossentropy' , metrics = [ 'acc' ]) # Entrenar el modelo history = model . fit ( x_train , y_train , epochs = 10 , batch_size = batch_size , validation_split = 0.2 ) # Evaluar el modelo test_loss , test_acc = model . evaluate ( x_test , y_test ) print ( 'Test accuracy:' , test_acc ) # Guardar el modelo model . save ( 'sentiment_model1.h5' ) # Probar con otro conjunto de datos # Cargar el modelo # Cargar el modelo previamente entrenado model = tf . keras . models . load_model ( '/content/rnn_sentiment_model.h5' ) # Reemplaza 'rnn_sentiment_model.h5' con el nombre de tu archivo de modelo # Imprimir las 10 primeras listas del conjunto de entrenamiento for i in range ( 10 ): print ( f \"Secuencia { i + 1 } : { x_train [ i ] } \" ) # Tus propias revisiones de pel\u00edculas reviews = [ \"This movie was fantastic\" , \"solid performances in this straightforward adaption\" , \"brilliant documentary\" , \"worst mistake of my life\" , ] # Tokeniza tus revisiones utilizando el mismo Tokenizer que usaste para el modelo tokenizer = Tokenizer ( num_words = 10000 ) # Aseg\u00farate de que coincida con el valor que usaste para entrenar el modelo tokenizer . fit_on_texts ( reviews ) sequences = tokenizer . texts_to_sequences ( reviews ) # Aseg\u00farate de que todas las secuencias tengan la misma longitud max_sequence_length = 100 # Aseg\u00farate de que coincida con el valor que usaste para entrenar el modelo sequences = pad_sequences ( sequences , maxlen = max_sequence_length ) # Realiza predicciones en tus propias revisiones predictions = model . predict ( sequences ) # Muestra las predicciones for i , prediction in enumerate ( predictions ): sentiment = \"positivo\" if prediction > 0.5 else \"negativo\" print ( f \"Sentimiento de tu revisi\u00f3n { i + 1 } : { sentiment } (Puntuaci\u00f3n: { prediction [ 0 ] : .4f } )\" )","title":"Redes Neuronales Recurrentes (RNR)"},{"location":"p5.html#redes-neuronales-convolucionales","text":"La convoluci\u00f3n es una operaci\u00f3n matem\u00e1tica que se utiliza para extraer caracter\u00edsticas importantes de los datos. Se utiliza mucho en el tratamiento de im\u00e1genes. Las CNN est\u00e1n formadas por tres tipos principales de capas: Capa convolucional: Es la primera capa de una red convolucional. Realiza la mayor\u00eda de los c\u00e1lculos y se encarga de identificar caracter\u00edsticas locales. Capa de activaci\u00f3n: a\u00f1aden funciones no lineales para permitir la aproximaci\u00f3n de funciones m\u00e1s complejas. Capa de agrupaci\u00f3n (pooling): Reduce la dimensionalidad de los datos y extrae caracter\u00edsticas m\u00e1s complejas. Capa totalmente conectada: La \u00faltima capa que procesa la informaci\u00f3n y produce la salida final. A medida que los datos de la imagen avanzan a trav\u00e9s de estas capas, la CNN reconoce elementos o formas m\u00e1s grandes, hasta identificar el objeto deseado. En el PLN se utilizan estas redes para aprender caracter\u00edsticas locales y globales del texto. from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense , Embedding , Conv1D , GlobalMaxPooling1D Construimos la red de forma secuencial red = Sequential () A\u00f1adimos la capa de embedding red . add ( Embedding ( tamano_vocabulario , 128 )) A\u00f1adimos una capa de convoluciones con 64 filtros convolucionales de tama\u00f1o 5. red . add ( Conv1D ( 64 , 5 , activation = 'relu' )) Reducimos la dimensionalidad con pooling red . add ( GlobalMaxPooling1D ()) A\u00f1adimos una capa densa de 32 neuronas para procesar las representaciones obtenidas. red . add ( Dense ( 32 )) Por \u00faltimo haremos que nuestra red termine en una \u00fanica neurona con una funci\u00f3n de activaci\u00f3n sigmoidal que har\u00e1 que los resultados est\u00e9n pr\u00f3ximos a los extremos, es decir, a 0 o a 1. Al final lo que queremos es que la red nos clasifique las rese\u00f1as como positivas o negativas red . add ( Dense ( 1 , activation = 'sigmoid' )) Le indicamos el optimizador que debe usar para reducir la p\u00e9rdida y el tipo de m\u00e9trica red . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) Entrenamos el modelo red . fit ( x_train , y_train , batch_size = 32 , epochs = 5 , validation_data = ( x_test , y_test )) Evaluamos el modelo loss , accuracy = red . evaluate ( x_test , y_test ) print ( f 'Tasa de acierto: { accuracy } ' )","title":"Redes Neuronales Convolucionales"},{"location":"p6.html","text":"TRANSFORMERS \u00b6 Es un tipo de arquitectura basada en redes neuronales que procesan secuencias de palabras y tienen memoria a largo plazo, es decir que logran analizar secuencias muy extensas usando un mecanismo que se llama atenci\u00f3n . Adem\u00e1s, procesan toda la secuencia en paralelo (las redes recurrentes lo hacen en serie). Las redes transformers est\u00e1n descritas en el art\u00edculo del 2017 llamado \"Attention is All You Need\". En lugar de tratar todas las palabras de manera uniforme, los mecanismos de atenci\u00f3n asignen pesos diferentes a cada palabra de entrada, de forma que las palabras relevantes acaben obteniendo m\u00e1s peso. Un transformer es un tipo de red neuronal que se basa exclusivamente en mecanismos de atenci\u00f3n para procesar datos de secuencia como textos, im\u00e1genes o series temporales. Realmente el ENCODERS es un conjunto de encoders, uno encima de otro. Lo mismo sucede con el DECODERS . La estructura de los encoders y los decoders es la siguiente: Introducci\u00f3n a los transformers \u00b6 Los transformers forman parte del pre-entrenamiento y potencian las t\u00e9cnicas que ya conocemos. \" Attention Is All You Need \" La arquitectura de los transformers da importancia a las relaciones entre palabras que no est\u00e1n cerca para generar un texto preciso y coherente. Los componentes de la arquitectura transformers es la siguiente: Preprocesamiento. Codificaci\u00f3n posicional (Posicional Encoding). Codificadores (Encoders). Decodificadores (Decoders). Hugging Face \u00b6 La biblioteca \"Transformers\" de Hugging Face ofrece una variedad de modelos de lenguaje preentrenados basados en la arquitectura transformer. Algunos de los modelos m\u00e1s populares incluyen BERT, DistilBERT y RoBERTa. BERT, que significa \"Bidirectional Encoder Representations from Transformers\", fue uno de los primeros modelos en aplicar la arquitectura transformer a tareas de PLN. Ofrece variantes como BERT base y BERT large, que difieren en el tama\u00f1o y la capacidad de procesamiento. DistilBERT es una versi\u00f3n m\u00e1s liviana de BERT que conserva gran parte de su rendimiento y eficacia, pero con menos par\u00e1metros y un menor tiempo de entrenamiento. Esta versi\u00f3n es perfecta para aplicaciones donde se requiere una buena relaci\u00f3n entre eficiencia y precisi\u00f3n. RoBERTa, una versi\u00f3n mejorada de BERT, utiliza t\u00e9cnicas de entrenamiento adicionales para mejorar a\u00fan m\u00e1s el rendimiento en tareas de PLN. Al igual que BERT, tambi\u00e9n est\u00e1 disponible en versiones base y large. Estos modelos de lenguaje ofrecen beneficios significativos para una variedad de tareas de PLN, como la clasificaci\u00f3n de secuencias, la generaci\u00f3n de texto y el an\u00e1lisis de sentimientos. Repaso de transformers \u00b6 La arquitectura de Transformers utiliza Encoders y Decoders. El Transformer permite entrenar en paralelo y aprovechar la GPU. Utiliza un mecanismo de atenci\u00f3n que cruza en memoria \u201ctodos contra todos los tokens\u201d y obtiene un score. Los modelos anteriores como LSTM no pod\u00edan memorizar textos largos ni correr en paralelo. El mecanismo de atenci\u00f3n puede ser de Self Attention en el Encoder, Cross Attention \u00f3 Masked Attention en el Decoder. Se utiliza El Input pero tambi\u00e9n la Salida (el Output del dataset) para entrenar al modelo.","title":"6. Transformers"},{"location":"p6.html#transformers","text":"Es un tipo de arquitectura basada en redes neuronales que procesan secuencias de palabras y tienen memoria a largo plazo, es decir que logran analizar secuencias muy extensas usando un mecanismo que se llama atenci\u00f3n . Adem\u00e1s, procesan toda la secuencia en paralelo (las redes recurrentes lo hacen en serie). Las redes transformers est\u00e1n descritas en el art\u00edculo del 2017 llamado \"Attention is All You Need\". En lugar de tratar todas las palabras de manera uniforme, los mecanismos de atenci\u00f3n asignen pesos diferentes a cada palabra de entrada, de forma que las palabras relevantes acaben obteniendo m\u00e1s peso. Un transformer es un tipo de red neuronal que se basa exclusivamente en mecanismos de atenci\u00f3n para procesar datos de secuencia como textos, im\u00e1genes o series temporales. Realmente el ENCODERS es un conjunto de encoders, uno encima de otro. Lo mismo sucede con el DECODERS . La estructura de los encoders y los decoders es la siguiente:","title":"TRANSFORMERS"},{"location":"p6.html#introduccion-a-los-transformers","text":"Los transformers forman parte del pre-entrenamiento y potencian las t\u00e9cnicas que ya conocemos. \" Attention Is All You Need \" La arquitectura de los transformers da importancia a las relaciones entre palabras que no est\u00e1n cerca para generar un texto preciso y coherente. Los componentes de la arquitectura transformers es la siguiente: Preprocesamiento. Codificaci\u00f3n posicional (Posicional Encoding). Codificadores (Encoders). Decodificadores (Decoders).","title":"Introducci\u00f3n a los transformers"},{"location":"p6.html#hugging-face","text":"La biblioteca \"Transformers\" de Hugging Face ofrece una variedad de modelos de lenguaje preentrenados basados en la arquitectura transformer. Algunos de los modelos m\u00e1s populares incluyen BERT, DistilBERT y RoBERTa. BERT, que significa \"Bidirectional Encoder Representations from Transformers\", fue uno de los primeros modelos en aplicar la arquitectura transformer a tareas de PLN. Ofrece variantes como BERT base y BERT large, que difieren en el tama\u00f1o y la capacidad de procesamiento. DistilBERT es una versi\u00f3n m\u00e1s liviana de BERT que conserva gran parte de su rendimiento y eficacia, pero con menos par\u00e1metros y un menor tiempo de entrenamiento. Esta versi\u00f3n es perfecta para aplicaciones donde se requiere una buena relaci\u00f3n entre eficiencia y precisi\u00f3n. RoBERTa, una versi\u00f3n mejorada de BERT, utiliza t\u00e9cnicas de entrenamiento adicionales para mejorar a\u00fan m\u00e1s el rendimiento en tareas de PLN. Al igual que BERT, tambi\u00e9n est\u00e1 disponible en versiones base y large. Estos modelos de lenguaje ofrecen beneficios significativos para una variedad de tareas de PLN, como la clasificaci\u00f3n de secuencias, la generaci\u00f3n de texto y el an\u00e1lisis de sentimientos.","title":"Hugging Face"},{"location":"p6.html#repaso-de-transformers","text":"La arquitectura de Transformers utiliza Encoders y Decoders. El Transformer permite entrenar en paralelo y aprovechar la GPU. Utiliza un mecanismo de atenci\u00f3n que cruza en memoria \u201ctodos contra todos los tokens\u201d y obtiene un score. Los modelos anteriores como LSTM no pod\u00edan memorizar textos largos ni correr en paralelo. El mecanismo de atenci\u00f3n puede ser de Self Attention en el Encoder, Cross Attention \u00f3 Masked Attention en el Decoder. Se utiliza El Input pero tambi\u00e9n la Salida (el Output del dataset) para entrenar al modelo.","title":"Repaso de transformers"},{"location":"p7.html","text":"TRANSFER LEARNING \u00b6 las t\u00e9cnicas de Aprendizaje Transferible (Transfer Learning) y el y Ajuste Fino (Fine Tuning) se presentan como innovaciones clave para la mejora de los modelos de Inteligencia Artificial. TL consiste en aprovechar los conocimientos adquiridos por un modelo entrenado para una determinada tarea para mejorar el rendimiento en otra tarea diferente, es decir que en lugar de entrenar un modelo desde cero para cada tarea, se transfieren los conocimientos previos de otro modelo con el fin de reducir la cantidad de datos necesarios para entrenar. TL implica congelar los pesos del modelo previamente entrenado y solo entrenar las nuevas capas, el fine tuning va un paso m\u00e1s all\u00e1 al permitir que se actualicen las capas previamente entrenadas. Ventajas del TL \u00b6 Reducci\u00f3n de costes de entrenamiento. Mejora del rendimiento con pocos datos. Mejoras en la generalizaci\u00f3n. Modelos GPT \u00b6 Los modelos GPT (Transformers Generativos Preentrenados) son importantes por su capacidad para capturar relaciones entre secuencias de datos. Se preentrenan de manera no supervisada sobre grandes cantidades de datos y una vez preentrenados, estos modelos se toman como base para el desarrollo de distintas tareas. Una de las aplicaciones m\u00e1s conocidas es ChatGPT que utiliza Transfer Learning. Fases de los modelos GPT \u00b6 Preentrenamiento : El modelo se preentrena poniendo como tarea predecir la siguiente palabra en una secuencia de texto dada una secuencdia de palabras anteriores. De esta manera, va aprendiendo c\u00f3mo se relacionan las palabras entre ellas tanto de forma sem\u00e1ntica como de forma sint\u00e1ctica. Ajuste de par\u00e1metros : Cuando el modelo ya ha aprendido sobre grandes cantidades de datos, se hace un \u00faltimo ajuste para adaptarlo a una tarea concreta. Adaptaci\u00f3n de los modelos \u00b6 Selecci\u00f3n de modelo base adecuado. (ejm: el idioma). Entrenamiento espec\u00edfico con datos relevantes. Ajuste de hiperpar\u00e1metros (para optimizar resultados). Evaluaci\u00f3n adecuada del modelo (m\u00e9tricas adecuadas). Transferencia de conocimiento gradual. Herramientas preentrenadas \u00b6 Hay diferentes herramientas y m\u00f3dulos de Python que ofrencen funcionalidades relacionadas con el PLN. Estas herramientas parten de modelos que ya est\u00e1n entrenados y nos permite poder utilizarlas en otras tareas. GENSIN es una biblioteca de python que realiza tareas como la extracci\u00f3n de temas en grandes vol\u00famenes de texto, c\u00e1lculo de siilitud de documentos o generaci\u00f3n de res\u00famenes. SUMY tambi\u00e9n es una biblioteca de python que implementa varios algoritmos para generar res\u00famenes de textos. Dispone de algoritmos ya entrenados para esta tarea como LexRank, TextRank o LSA. BART es un modelo de lenguaje avanzado de Hugging Face que genera texto, traduce, corrige de forma autom\u00e1tica y convierte texto a voz y viceversa. BERTSUM creado a partir de BERT y entrenado espec\u00edficamente mediante transfer learning para la generaci\u00f3n autom\u00e1tica de res\u00famenes. TEXTTEASER es una herramienta de pytnon para la generaci\u00f3n autom\u00e1tica de res\u00famenes. KEYBERT implementado por Hugging Face para la realizaci\u00f3n de tareas de resumen y extracci\u00f3n de palabras clave.","title":"7. Transfer Learning"},{"location":"p7.html#transfer-learning","text":"las t\u00e9cnicas de Aprendizaje Transferible (Transfer Learning) y el y Ajuste Fino (Fine Tuning) se presentan como innovaciones clave para la mejora de los modelos de Inteligencia Artificial. TL consiste en aprovechar los conocimientos adquiridos por un modelo entrenado para una determinada tarea para mejorar el rendimiento en otra tarea diferente, es decir que en lugar de entrenar un modelo desde cero para cada tarea, se transfieren los conocimientos previos de otro modelo con el fin de reducir la cantidad de datos necesarios para entrenar. TL implica congelar los pesos del modelo previamente entrenado y solo entrenar las nuevas capas, el fine tuning va un paso m\u00e1s all\u00e1 al permitir que se actualicen las capas previamente entrenadas.","title":"TRANSFER LEARNING"},{"location":"p7.html#ventajas-del-tl","text":"Reducci\u00f3n de costes de entrenamiento. Mejora del rendimiento con pocos datos. Mejoras en la generalizaci\u00f3n.","title":"Ventajas del TL"},{"location":"p7.html#modelos-gpt","text":"Los modelos GPT (Transformers Generativos Preentrenados) son importantes por su capacidad para capturar relaciones entre secuencias de datos. Se preentrenan de manera no supervisada sobre grandes cantidades de datos y una vez preentrenados, estos modelos se toman como base para el desarrollo de distintas tareas. Una de las aplicaciones m\u00e1s conocidas es ChatGPT que utiliza Transfer Learning.","title":"Modelos GPT"},{"location":"p7.html#fases-de-los-modelos-gpt","text":"Preentrenamiento : El modelo se preentrena poniendo como tarea predecir la siguiente palabra en una secuencia de texto dada una secuencdia de palabras anteriores. De esta manera, va aprendiendo c\u00f3mo se relacionan las palabras entre ellas tanto de forma sem\u00e1ntica como de forma sint\u00e1ctica. Ajuste de par\u00e1metros : Cuando el modelo ya ha aprendido sobre grandes cantidades de datos, se hace un \u00faltimo ajuste para adaptarlo a una tarea concreta.","title":"Fases de los modelos GPT"},{"location":"p7.html#adaptacion-de-los-modelos","text":"Selecci\u00f3n de modelo base adecuado. (ejm: el idioma). Entrenamiento espec\u00edfico con datos relevantes. Ajuste de hiperpar\u00e1metros (para optimizar resultados). Evaluaci\u00f3n adecuada del modelo (m\u00e9tricas adecuadas). Transferencia de conocimiento gradual.","title":"Adaptaci\u00f3n de los modelos"},{"location":"p7.html#herramientas-preentrenadas","text":"Hay diferentes herramientas y m\u00f3dulos de Python que ofrencen funcionalidades relacionadas con el PLN. Estas herramientas parten de modelos que ya est\u00e1n entrenados y nos permite poder utilizarlas en otras tareas. GENSIN es una biblioteca de python que realiza tareas como la extracci\u00f3n de temas en grandes vol\u00famenes de texto, c\u00e1lculo de siilitud de documentos o generaci\u00f3n de res\u00famenes. SUMY tambi\u00e9n es una biblioteca de python que implementa varios algoritmos para generar res\u00famenes de textos. Dispone de algoritmos ya entrenados para esta tarea como LexRank, TextRank o LSA. BART es un modelo de lenguaje avanzado de Hugging Face que genera texto, traduce, corrige de forma autom\u00e1tica y convierte texto a voz y viceversa. BERTSUM creado a partir de BERT y entrenado espec\u00edficamente mediante transfer learning para la generaci\u00f3n autom\u00e1tica de res\u00famenes. TEXTTEASER es una herramienta de pytnon para la generaci\u00f3n autom\u00e1tica de res\u00famenes. KEYBERT implementado por Hugging Face para la realizaci\u00f3n de tareas de resumen y extracci\u00f3n de palabras clave.","title":"Herramientas preentrenadas"}]}