# **Librerías para PLN**

## **NLTK: Natural Language Toolkit**

Es una librería de Python utilizada en el procesamiento del lenguaje natural. **NLTK** ofrece potentes capacidades de tokenización que facilitan el procesamiento eficiente de datos textuales.

La tokenización de palabras de NLTK le permite dividir el texto en palabras individuales o tokens, además contiene corpus de textos y vocabulario organizado como **WordNet**.


### **Métodos en NLTK**


* Nltk.chunk: Utiliza expresiones regulares o árboles de análisis para extraer frases.
* Nltk.tokenize: Divide el texto en frases, palabras o subpalabras, esencial para la estructura del texto.
* Nltk.corpus: Colección de textos que se utiliza como muestra del lenguaje real.
* Nltk.sentiment: Analiza el sentimiento de un texto, como positivo o negativo.
* Nltk.translate: Funciones y algoritmos para trabajar con traducción automática.
* Nltk.stem: Reduce las palabras a su raíz, eliminando sufijos y prefijos.
* Nltk.tag: Asigna partes del discurso a palabras como sustantivos, verbos, adjetivos, ...
* Nltk.inference: Herramienta para trabajar con lógica e inferencia en textos.

### ** Tokenización con NLTK**

Importamos la librería nltk y descargamos **punkt** que es el tokenizador de NLTK, el algoritmo que lleva a cabo la tokenización de las palabras que pases por parámetro. Divide el texto en frases a través de un modelo no supervisado, esto es que no necesita datos etiquetados para su entrenamiento, obtendrá patrones de los propios datos.

```python
import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
```
Dado un texto, podemos ver los tokens con el siguiente código:
```python
tokens = nltk.word_tokenize(texto)
```

Descargamos las stopwords

```python
nltk.download('stopwords')
```
Seleccionamos únicamente las stopwords en Español

```python
stop_words=set(stopwords.words('spanish'))
```

Tokenización
```python
tokens = word_tokenize(texto)
```

Sacamos únicamente las palabras que no están en las stop_words

```python
texto_filtrado=[word for word in tokens if not word in stop_words]
```

¿Qué sucede si no tenemos alguna stopword en mayúsculas? No las reconoce como stopwords por lo tanto lo primero que tenemos que hacer es pasar todo el texto a minúsculas.

```python
texto=texto.lower()
```
### ** Stemming con NLTK**


```python
nltk.download('wordnet')
```

```python
from nltk.stem import SnowballStemmer

# Creamos el stemmer en español
stemmer=SnowballStemmer('spanish')
```
Probamos con la palabra caminando

```python
print(stemmer.stem('caminando'))
```
### ** Lematización con NLTK**

Únicamente tiene el diccionario en Inglés.

```python
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
```
Inicializamos el tokenizador

```python
lematizador=WordNetLemmatizer()
```

```python
print(lematizador.lemmatize("running","v"))
```


  


## **Spacy**

Es otra librería de Python para PLN muy similar a NLTK y su implementación suele ser más rápida y precisa.

### **Métodos en Spacy**

* Spacy.load: Carga un modelo de lenguaje específico. Ejm: nlp=spacy.load('es_core_web_sm).
* nlp: Procesa un texto y ejecuta la cadena de procesamiento del modelo sobre él. 
* Doc, Span, Token: Proporciona contenedores para acceder a información sobre el texto como palabras individuales o grupos de palabras.
* Token.lemma_: Obtiene la forma base de una palabra.
* Doc.sents: Divide un documento en frases.
* Doc.ents: Extrae entidades nombradas como personas, organizaciones o lugares.

### **Lematización en Spacy**

Instalamos Spacy

```python
!pip install spacy -q
```
Descargamos los paquetes que necesitamos para la lematización.

```python
!python -m spacy download es_core_news_sm -q
```
Cargar el modelo en español
```python
import spacy
nlp=spacy.load('es_core_news_sm')
```
Probamos con texto (cualquier frase)

```python
doc=nlp('texto')
```
Imprimimos el texto y el lema de cada token
```python
for token in doc:
  print(token.text, '->', token.lemma_)
```
Podemos mostrar cada token que etiqueta tiene
```python
for token in doc:
  print(token.text, '->', token.pos_)
```

## Actividad 2.1

!!! note "Actividad 2.1"
    Crea un notebook dividido en dos partes:

    - NLTK

      1. Realiza la tokenización del texto: "El gato y el perro salen a jugar a la calle." Explica cada paso y visualiza los tokens finales.

      2. Comprobar el stemming de diferentes tiempos del verbo pasear. Explica el resultado.

      3. Haz la lematización de 5 palabras en inglés y explica el resultado.

    - Spacy

      1. Realiza la lematización de un texto en castellano y explica los resultados.

      2. Aplica alguno de los métodos de Spacy.

<!--
### **Análisis semántico con WordNet**

La filosofía de diseño de WordNet se apoya en una idea similar a la del conocido refrán "dime con quién andas y te diré quién eres". Una versión semántica de este refrán podría ser "dime a qué palabras se parece y te diré lo que significa". 

- Es una base de datos léxica usada para el idioma inglés.

- Ayuda a resolver el problema de la sinonimia.

- También dispone en otros idiomas, incluyendo el castellano (euroWordNet)

- Los items léxicos están categorizados en más de 115.000 synsets.

- Un synset consta de un conjunto de sinónimos, una definición de diccionario (glosa) y algunos ejemplos de uso.

- Las relaciones semánticas se representan como redes que las aplicaciones pueden recorrer para encontrar sinónimos, antónimos, ...

https://github.com/jatroyano/notebook-nltk-similitud/blob/master/nltk-similitud.ipynb

-->

<!--
### **ChatBot**

Importamos las librerías que necesitamos.

```python
import nltk
from nltk.chat.util import Chat, reflections
```
Definimos una función para una conversión básica.

```python
def chatbot():
  print('Hola, soy ChatBot, ¿en qué puedo ayudarte')
  pairs=[
      [r'(.*)mi nombre es(.*)',['Hola, como puedo ayudarte']],
      [r'(.*)cual es tu nombre(.*)',['mi nombre es chatbot']],
      [r'(.*)cómo estás(.*)',['Bien, muchas gracias, y tú']],
      [r'(.*)adios(.*)',['adios, que tengas un buen día']],
      [r'(.*)finanzas(.*)',['Cuéntame qué quieres saber de eso']],
      [r'(.*)empresa(.*)',['qué quieres saber de la empressa']]
  ]

  chat=Chat(pairs, reflections)
  chat.converse()
```
Descargamos el punkt.

``` python
if __name__=='__main__':
  nltk.download('punkt')
  chatbot()

```



continuar vídeo data&Analytics procesamiento del lenguaje natural, uso de bibliotecas como Nltk y spacy



### **Fion**

![imagen](imagenes/DP.PNG)

Tras la obtención de los datos desde los orígenes, es el momento de la preparación del dato, para normalizarlo según las necesidades y por supuesto, para optimizar al máximo el dato importado. Este paso es importante además de por el rendimiento en la posterior visualización, sino para garantizar un dato "limpio" y normalizado.

1. Limpieza de datos.
2. Verificación de datos.
3. Tipos de datos.
4. Formateo de los datos numéricos y fecha.
5. Definir resúmenes en columnas numéricas.
6. Establecer nombres lógicos para los campos.
7. Nombres de campo únicos en todo el conjunto de datos.
8. Cambiar el nombre de los pasos aplicados.
9. Particionar campos.



### **Fase 3: Dg**






### **Fase n**




### **Actividades**

1. Realiza un Chatbot con las siguientes características:
    - Dar las entradas y las salidas al script.
    - Debes preguntar algo y el bot debe responder.
    - Al decirle que no necesitas más su ayuda, el bot se despide.

2. 

!!! Note "Actividades"
     mnmnmn m nm nm mn mn 

     -->