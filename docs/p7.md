# **TRANSFER LEARNING**

Consiste en aprovechar los conocimientos adquiridos por un modelo entrenado para una determinada tarea para mejorar el rendimiento en otra tarea diferente, es decir que en lugar de entrenar un modelo desde cero para cada tarea, se transfieren los conocimientos previos de otro modelo con el fin de reducir la cantidad de datos necesarios para entrenar.



![imagen](imagenes/t1.png)




## **Ventajas del TL**

- Reducción de costes de entrenamiento.
- Mejora del rendimiento con pocos datos.
- Mejoras en la generalización.

## **Modelos GPT**

Los modelos GPT (Transformers Generativos Preentrenados) son importantes por su capacidad para capturar relaciones entre secuencias de datos. Se preentrenan de manera no supervisada sobre grandes cantidades de datos y una vez preentrenados, estos modelos se toman como base para el desarrollo de distintas tareas.

Una de las aplicaciones más conocidas es ChatGPT que utiliza Transfer Learning.


### **Fases de los modelos GPT**

- **Preentrenamiento**: El modelo se preentrena poniendo como tarea predecir la siguiente palabra en una secuencia de texto dada una secuencdia de palabras anteriores. De esta manera, va aprendiendo cómo se relacionan las palabras entre ellas tanto de forma semántica como de forma sintáctica.

-**Ajuste de parámetros**: Cuando el modelo ya ha aprendido sobre grandes cantidades de datos, se hace un último ajuste para adaptarlo a una tarea concreta.


### **Adaptación de los modelos**


1. Selección de modelo base adecuado. (ejm: el idioma).
2. Entrenamiento específico con datos relevantes. 
3. Ajuste de hiperparámetros (para optimizar resultados).
4. Evaluación adecuada del modelo (métricas adecuadas).
5. Transferencia de conocimiento gradual.

## **Herramientas preentrenadas**

Hay diferentes herramientas y módulos de Python que ofrencen funcionalidades relacionadas con el PLN. Estas herramientas parten de modelos que ya están entrenados y nos permite poder utilizarlas en otras tareas.

**GENSIN** es una biblioteca de python que realiza tareas como la extracción de temas en grandes volúmenes de texto, cálculo de siilitud de documentos o generación de resúmenes.

**SUMY** también es una biblioteca de python que implementa varios algoritmos para generar resúmenes de textos. Dispone de algoritmos ya entrenados para esta tarea como LexRank, TextRank o LSA.

**BART** es un modelo de lenguaje avanzado de Hugging Face que genera texto, traduce, corrige de forma automática y convierte texto a voz y viceversa.

**BERTSUM** creado a partir de BERT y entrenado específicamente mediante transfer learning para la generación automática de resúmenes.

**TEXTTEASER** es una herramienta de pytnon para la generación automática de resúmenes.

**KEYBERT** implementado por Hugging Face para la realización de tareas de resumen y extracción de palabras clave.


## **Repaso de transformers**


