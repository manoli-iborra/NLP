# **TRANSFORMERS**

Es un tipo de arquitectura basada en redes neuronales que procesan secuencias de palabras y tienen memoria a largo plazo, es decir que logran analizar secuencias muy extensas usando un mecanismo que se llama **atención**. Además, procesan toda la secuencia en paralelo (las redes recurrentes lo hacen en serie).

Las redes transformers están descritas en el artículo del 2017 llamado "Attention is All You Need".


## **PRE-ENTRENAMIENTO**

El pre-entrenamiento gnerativo proporciona al modelo un conjunto de datos formador por tokens y los entrena para predecir los tokens que contiene.

Hay dos formas de pre-entrenamiento:

* Predicción de la siguiente palabra.
* El modelo de lenguaje enmascarado.

### **Predicción de la siguiente palabra**

Es una técnica de aprendizaje supervisado que entrena el modelo con los datos de entrada y su correspondiente salida. Permite entrenar el modelo para predecir la siguiente palabra en una oración dado el contexto de las palabras anteriores.

El modelo aprende a generar texto coherente conociendo las dependencias entre palabras en un contexto más ámplio. Cuantos más ejemplos se ven, mejor se predice la siguiente palabra.

![imagen](imagenes/T1.PNG)

### **El modelo de lenguaje enmascarado**

El modelo de lenguaje enmascarado entrena un modelo para predecir una palabra que está oculta en una oración.

Durante el entrenamiento, el modelo recibe como entrada tanto el texto original como el enmascarado.

![imagen](imagenes/T2.PNG)

## **Introducción a los transformers**

Los transformers forman parte del pre-entrenamiento y potencian las técnicas que ya conocemos.

" Attention Is All You Need "

La arquitectura de los transformers da importancia a las relaciones entre palabras que no están cerca para generar un texto preciso y coherente.

Los componentes de la arquitectura transformers es la siguiente:

* Preprocesamiento.
* Codificación posicional (Posicional Encoding).
* Codificadores (Encoders).
* Decodificadores (Decoders).

