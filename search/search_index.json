{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Procesamiento del Lenguaje Natural (PLN) \u00b6 Introducci\u00f3n \u00b6 El Procesamiento del Lenguaje Natural (Natural Languaje Processing, NLP) es un \u00e1rea de la Inteligencia Artificial (IA) relacionada con el procesamiento, an\u00e1lisis y comprensi\u00f3n de cualquier lenguaje natural con el objetivo de crear sistemas basados en IA que sean capaces de interactuar con seres humanos en el mismo lenguaje de manera escrita o hablada. El PLN se encuentra en la zona intermedia entre la ling\u00fc\u00edstica y la IA. Un sistema de PLN puede actuar en tres escenarios diferentes: Sistemas en los que el texto es la entrada (input) del sistema. Sistemas en los que el texto es la salida (output) del sistema. Sistemas en los que el texto es tanto la entrada como la salida. \u00c1reas de aplicaci\u00f3n del PLN \u00b6 Las \u00e1reas del PLN son: NLP: Procesamiento del Lenguaje Natural. NLU: Entendimiento del Lenguaje Natural. NLG: Generaci\u00f3n del Lenguaje Natural. Aplicaciones del PLN \u00b6 Recuperaci\u00f3n de informaci\u00f3n (Information Retrieval) Extracci\u00f3n de informaci\u00f3n (Information Extraction) QA, Respuestas a preguntas (Question Answering) Clasificaci\u00f3n textual (Text Clasification) NLG, generaci\u00f3n de lenguaje natural (Natural Languaje Generation) Conceptos importantes \u00b6 Unidades de comunicaci\u00f3n b\u00e1sica \u00b6 Desde el punto de vista computacional, en vez de trabajar con palabras, se trabaja con el concepto de token . Un token es una secuencia de caracteres separados por espacio en blanco. La tokenizaci\u00f3n sirve como base para varias tareas de NLP, como la clasificaci\u00f3n de texto, el an\u00e1lisis de sentimientos y el reconocimiento de entidades con nombre. Consiste en dividir el texto unidades m\u00e1s peque\u00f1as llamadas tokens. Todos los tokens de un texto que sean iguales se considera que pertenecen al mismo type o tipo. Si en un texto aparece, por ejemplo, cinco veces la palabra amigo, se dice que son cinco tokens del type amigo. Ejemplo: Es amigo de mi amigo Types : es amigo de mi (cuatro types) Tokens : es amigo de mi amigo (cinco tokens) El lema es la forma de nombrar una palabra y todas sus derivaciones morfol\u00f3gicas. En el caso de los verbos, el lema suele ser la forma de infinitivo y en el caso de los nombres, la forma de masculino singular. Si por ejemplo aparece en un texto types como \"cantar\u00eda\", \"cant\u00e1bamos\", \"cantar\u00e9\", etc. Se dice que todos sus tokens pertenecen al lema \"cantar\". A este proceso se le llama lematizaci\u00f3n . El stemming consiste en reducir cada token a su raiz o lexema: la parte invariable que asume el significado de la palabra. Si tengo la palabra caminando, elimian \"ando\" y deja \"camin\". Proyector de embeddings Vectores \u00b6 Los vectores son representaciones num\u00e9ricas de texto, es decir es la forma en que traducimos el texto a un formato que un modelo de aprendizaje autom\u00e1tico puede entender y aprender. El objetivo es tener representaciones vectoriales \u00fatiles. Stop Words \u00b6 Son palabras que no dan mucho significado a la oraci\u00f3n (y, el, es, ...). Aparecen muchas veces en todas las oraciones pero no aportan mucha informaci\u00f3n. Si ignoramos estas stop words, los vectores van a tener menos dimensiones y se va a poder trabajar mejor. Corpus \u00b6 Un corpus ling\u00fc\u00edstico es un conjunto de textos relativamente grande, que refleja una lengua. Es decir, es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Encontrar un buen corpus sobre el cual trabajar no suele ser una tarea sencilla; uno que se suele utilizar para entrenar modelos es la informaci\u00f3n de wikipedia.","title":"1. Procesamiento del Lenguaje Natural"},{"location":"index.html#procesamiento-del-lenguaje-natural-pln","text":"","title":"Procesamiento del Lenguaje Natural (PLN)"},{"location":"index.html#introduccion","text":"El Procesamiento del Lenguaje Natural (Natural Languaje Processing, NLP) es un \u00e1rea de la Inteligencia Artificial (IA) relacionada con el procesamiento, an\u00e1lisis y comprensi\u00f3n de cualquier lenguaje natural con el objetivo de crear sistemas basados en IA que sean capaces de interactuar con seres humanos en el mismo lenguaje de manera escrita o hablada. El PLN se encuentra en la zona intermedia entre la ling\u00fc\u00edstica y la IA. Un sistema de PLN puede actuar en tres escenarios diferentes: Sistemas en los que el texto es la entrada (input) del sistema. Sistemas en los que el texto es la salida (output) del sistema. Sistemas en los que el texto es tanto la entrada como la salida.","title":"Introducci\u00f3n"},{"location":"index.html#areas-de-aplicacion-del-pln","text":"Las \u00e1reas del PLN son: NLP: Procesamiento del Lenguaje Natural. NLU: Entendimiento del Lenguaje Natural. NLG: Generaci\u00f3n del Lenguaje Natural.","title":"\u00c1reas de aplicaci\u00f3n del PLN"},{"location":"index.html#aplicaciones-del-pln","text":"Recuperaci\u00f3n de informaci\u00f3n (Information Retrieval) Extracci\u00f3n de informaci\u00f3n (Information Extraction) QA, Respuestas a preguntas (Question Answering) Clasificaci\u00f3n textual (Text Clasification) NLG, generaci\u00f3n de lenguaje natural (Natural Languaje Generation)","title":"Aplicaciones del PLN"},{"location":"index.html#conceptos-importantes","text":"","title":"Conceptos importantes"},{"location":"index.html#unidades-de-comunicacion-basica","text":"Desde el punto de vista computacional, en vez de trabajar con palabras, se trabaja con el concepto de token . Un token es una secuencia de caracteres separados por espacio en blanco. La tokenizaci\u00f3n sirve como base para varias tareas de NLP, como la clasificaci\u00f3n de texto, el an\u00e1lisis de sentimientos y el reconocimiento de entidades con nombre. Consiste en dividir el texto unidades m\u00e1s peque\u00f1as llamadas tokens. Todos los tokens de un texto que sean iguales se considera que pertenecen al mismo type o tipo. Si en un texto aparece, por ejemplo, cinco veces la palabra amigo, se dice que son cinco tokens del type amigo. Ejemplo: Es amigo de mi amigo Types : es amigo de mi (cuatro types) Tokens : es amigo de mi amigo (cinco tokens) El lema es la forma de nombrar una palabra y todas sus derivaciones morfol\u00f3gicas. En el caso de los verbos, el lema suele ser la forma de infinitivo y en el caso de los nombres, la forma de masculino singular. Si por ejemplo aparece en un texto types como \"cantar\u00eda\", \"cant\u00e1bamos\", \"cantar\u00e9\", etc. Se dice que todos sus tokens pertenecen al lema \"cantar\". A este proceso se le llama lematizaci\u00f3n . El stemming consiste en reducir cada token a su raiz o lexema: la parte invariable que asume el significado de la palabra. Si tengo la palabra caminando, elimian \"ando\" y deja \"camin\". Proyector de embeddings","title":"Unidades de comunicaci\u00f3n b\u00e1sica"},{"location":"index.html#vectores","text":"Los vectores son representaciones num\u00e9ricas de texto, es decir es la forma en que traducimos el texto a un formato que un modelo de aprendizaje autom\u00e1tico puede entender y aprender. El objetivo es tener representaciones vectoriales \u00fatiles.","title":"Vectores"},{"location":"index.html#stop-words","text":"Son palabras que no dan mucho significado a la oraci\u00f3n (y, el, es, ...). Aparecen muchas veces en todas las oraciones pero no aportan mucha informaci\u00f3n. Si ignoramos estas stop words, los vectores van a tener menos dimensiones y se va a poder trabajar mejor.","title":"Stop Words"},{"location":"index.html#corpus","text":"Un corpus ling\u00fc\u00edstico es un conjunto de textos relativamente grande, que refleja una lengua. Es decir, es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Encontrar un buen corpus sobre el cual trabajar no suele ser una tarea sencilla; uno que se suele utilizar para entrenar modelos es la informaci\u00f3n de wikipedia.","title":"Corpus"},{"location":"p.html","text":"POWER BI \u00b6 Microsoft Power BI es la soluci\u00f3n destinada a la inteligencia empresarial, que permite unir m\u00e1s de 65 fuentes de datos diferentes, para el modelado y posterior an\u00e1lisis de los datos, as\u00ed como para presentarlos a trav\u00e9s de paneles e informes. Caracter\u00edsticas de Power BI \u00b6 Est\u00e1 integrada en el ecosistema de Microsoft, por lo que permite acceder de forma sencilla a elementos almacenados en Sharepoint o Onedrive, as\u00ed como conectar otras herrammientas del mismo ecosistema a Power BI de una manera muy intutitiva. Con los paneles de informes (cuadros de mando o Dashboards) se consigue que los datos puedan ser consultados por los usuarios de una manera muy f\u00e1cil, atractiva e intuitiva. Agilidad en la interpretaci\u00f3n de los datos y la interacci\u00f3n de estos entre las diferentes gr\u00e1ficas y tablas representadas. Los paneles pueden ser compartidos por muchos usuarios de una misma empresa u organizaci\u00f3n. Se puede disponer de una manera \u00e1gil de informaci\u00f3n sobre sus negocios en tiempo real. Seguridad en la visualizaci\u00f3n de la informaci\u00f3n ya que se pueden asignar permisos de visualizaci\u00f3n de los cuadros de mando. Tiene un \u00e1mplio repertorio de objetos visuales adem\u00e1s de los que dispone por defecto en la propia herramienta. Gran comunidad de soporte que facilitan la resoluci\u00f3n de problemas y dudas que puedan surgir en el camino de aprendizaje. Todos los meses se a\u00f1aden mejoras o se optimizan los recursos de la herramienta. Licencias de Power BI \u00b6 Fases para la generaci\u00f3n de Dashboards \u00b6 Fase 1: Get Data \u00b6 Get Data = Obtener datos, realizar consultas (Query), importar datos, conexi\u00f3n a datos, etc. Los datos residen en diferentes bases de datos y archivos (Microsoft SQL Server, excel, csv, etc). El primer paso es detectar cuales son los or\u00edgenes de datos a los que se debe conectar Power BI. El asistente de la propia conexi\u00f3n seg\u00fan los par\u00e1metros necesarios para conectar, guiar\u00e1 al usuario mediante unos pasos muy sencillos y expl\u00edcitos. Este primer paso es importante ya que condicionar\u00e1 los tiempos de presentaci\u00f3n en los cuadros de mando, ya que la importaci\u00f3n de contenido irrelevante para las necesidades del proyecto, provocar\u00e1 un aumento en los tiempos de carga, que se arrastrar\u00e1 desde ese momento en adelante. Fase 2: Data Preparation \u00b6 Tras la obtenci\u00f3n de los datos desde los or\u00edgenes, es el momento de la preparaci\u00f3n del dato, para normalizarlo seg\u00fan las necesidades y por supuesto, para optimizar al m\u00e1ximo el dato importado. Este paso es importante adem\u00e1s de por el rendimiento en la posterior visualizaci\u00f3n, sino para garantizar un dato \"limpio\" y normalizado. Limpieza de datos. Verificaci\u00f3n de datos. Tipos de datos. Formateo de los datos num\u00e9ricos y fecha. Definir res\u00famenes en columnas num\u00e9ricas. Establecer nombres l\u00f3gicos para los campos. Nombres de campo \u00fanicos en todo el conjunto de datos. Cambiar el nombre de los pasos aplicados. Particionar campos. Existe un motor dise\u00f1ado para estas funciones que se llama Power Query y utiliza el lenguaje \"M\". Fase 3: Data Modeling \u00b6 En el proceso de modelado, es donde se relacionan las tablas de datos cargadas, de forma que se pueda acceder a datos de diferenes tablas, partiendo de un valor com\u00fan entre tablas. En esta fase es donde se realizan las medidas y columnas calculadas necesarias para complementar el contenido que se necesita, seg\u00fan los KPIs especificados. Un modelo de datos es la recopilaci\u00f3n de: Tablas de datos cargadas. Relaciones entre las tablas cargadas. Las medidas (f\u00f3rmulas) escritas que aplican las KPIs. Fase 4: Data Visualization \u00b6 Es la fase del desarrollo de los cuadros de mando, donde van a estar representados los KPIs definidos de una forma gr\u00e1fica o con contenido muy resumido de forma que no sea complicada su lectura, a fin de conseguir poder tener una capacidad alta en la toma de decisiones, a partir de los datos representados. Fase 5: Data Reporting \u00b6 Consiste en publicar el contenido que se acaba de dise\u00f1ar, con el fin de que el acceso a este no quede restringido a nuestro equipo, pudiendo permitir el acceso a diferentes usuarios dentro de la corporaci\u00f3n. Este proceso es muy importante a nivel de seguridad, ya que va a ser donde ser realicen las restricciones, tanto de acceso a los tableros, como de acceso a los datos y asignaci\u00f3n de roles.","title":"**POWER BI**"},{"location":"p.html#power-bi","text":"Microsoft Power BI es la soluci\u00f3n destinada a la inteligencia empresarial, que permite unir m\u00e1s de 65 fuentes de datos diferentes, para el modelado y posterior an\u00e1lisis de los datos, as\u00ed como para presentarlos a trav\u00e9s de paneles e informes.","title":"POWER BI"},{"location":"p.html#caracteristicas-de-power-bi","text":"Est\u00e1 integrada en el ecosistema de Microsoft, por lo que permite acceder de forma sencilla a elementos almacenados en Sharepoint o Onedrive, as\u00ed como conectar otras herrammientas del mismo ecosistema a Power BI de una manera muy intutitiva. Con los paneles de informes (cuadros de mando o Dashboards) se consigue que los datos puedan ser consultados por los usuarios de una manera muy f\u00e1cil, atractiva e intuitiva. Agilidad en la interpretaci\u00f3n de los datos y la interacci\u00f3n de estos entre las diferentes gr\u00e1ficas y tablas representadas. Los paneles pueden ser compartidos por muchos usuarios de una misma empresa u organizaci\u00f3n. Se puede disponer de una manera \u00e1gil de informaci\u00f3n sobre sus negocios en tiempo real. Seguridad en la visualizaci\u00f3n de la informaci\u00f3n ya que se pueden asignar permisos de visualizaci\u00f3n de los cuadros de mando. Tiene un \u00e1mplio repertorio de objetos visuales adem\u00e1s de los que dispone por defecto en la propia herramienta. Gran comunidad de soporte que facilitan la resoluci\u00f3n de problemas y dudas que puedan surgir en el camino de aprendizaje. Todos los meses se a\u00f1aden mejoras o se optimizan los recursos de la herramienta.","title":"Caracter\u00edsticas de Power BI"},{"location":"p.html#licencias-de-power-bi","text":"","title":"Licencias de Power BI"},{"location":"p.html#fases-para-la-generacion-de-dashboards","text":"","title":"Fases para la generaci\u00f3n de Dashboards"},{"location":"p.html#fase-1-get-data","text":"Get Data = Obtener datos, realizar consultas (Query), importar datos, conexi\u00f3n a datos, etc. Los datos residen en diferentes bases de datos y archivos (Microsoft SQL Server, excel, csv, etc). El primer paso es detectar cuales son los or\u00edgenes de datos a los que se debe conectar Power BI. El asistente de la propia conexi\u00f3n seg\u00fan los par\u00e1metros necesarios para conectar, guiar\u00e1 al usuario mediante unos pasos muy sencillos y expl\u00edcitos. Este primer paso es importante ya que condicionar\u00e1 los tiempos de presentaci\u00f3n en los cuadros de mando, ya que la importaci\u00f3n de contenido irrelevante para las necesidades del proyecto, provocar\u00e1 un aumento en los tiempos de carga, que se arrastrar\u00e1 desde ese momento en adelante.","title":"Fase 1: Get Data"},{"location":"p.html#fase-2-data-preparation","text":"Tras la obtenci\u00f3n de los datos desde los or\u00edgenes, es el momento de la preparaci\u00f3n del dato, para normalizarlo seg\u00fan las necesidades y por supuesto, para optimizar al m\u00e1ximo el dato importado. Este paso es importante adem\u00e1s de por el rendimiento en la posterior visualizaci\u00f3n, sino para garantizar un dato \"limpio\" y normalizado. Limpieza de datos. Verificaci\u00f3n de datos. Tipos de datos. Formateo de los datos num\u00e9ricos y fecha. Definir res\u00famenes en columnas num\u00e9ricas. Establecer nombres l\u00f3gicos para los campos. Nombres de campo \u00fanicos en todo el conjunto de datos. Cambiar el nombre de los pasos aplicados. Particionar campos. Existe un motor dise\u00f1ado para estas funciones que se llama Power Query y utiliza el lenguaje \"M\".","title":"Fase 2: Data Preparation"},{"location":"p.html#fase-3-data-modeling","text":"En el proceso de modelado, es donde se relacionan las tablas de datos cargadas, de forma que se pueda acceder a datos de diferenes tablas, partiendo de un valor com\u00fan entre tablas. En esta fase es donde se realizan las medidas y columnas calculadas necesarias para complementar el contenido que se necesita, seg\u00fan los KPIs especificados. Un modelo de datos es la recopilaci\u00f3n de: Tablas de datos cargadas. Relaciones entre las tablas cargadas. Las medidas (f\u00f3rmulas) escritas que aplican las KPIs.","title":"Fase 3: Data Modeling"},{"location":"p.html#fase-4-data-visualization","text":"Es la fase del desarrollo de los cuadros de mando, donde van a estar representados los KPIs definidos de una forma gr\u00e1fica o con contenido muy resumido de forma que no sea complicada su lectura, a fin de conseguir poder tener una capacidad alta en la toma de decisiones, a partir de los datos representados.","title":"Fase 4: Data Visualization"},{"location":"p.html#fase-5-data-reporting","text":"Consiste en publicar el contenido que se acaba de dise\u00f1ar, con el fin de que el acceso a este no quede restringido a nuestro equipo, pudiendo permitir el acceso a diferentes usuarios dentro de la corporaci\u00f3n. Este proceso es muy importante a nivel de seguridad, ya que va a ser donde ser realicen las restricciones, tanto de acceso a los tableros, como de acceso a los datos y asignaci\u00f3n de roles.","title":"Fase 5: Data Reporting"},{"location":"p1.html","text":"**Librer\u00edas para PLN \u00b6 NLTK: Natural Language Toolkit \u00b6 Es una librer\u00eda de Python utilizada en el procesamiento del lenguaje natural. NLTK ofrece potentes capacidades de tokenizaci\u00f3n que facilitan el procesamiento eficiente de datos textuales. La tokenizaci\u00f3n de palabras de NLTK le permite dividir el texto en palabras individuales o tokens, adem\u00e1s contiene corpus de textos y vocabulario organizado como WordNet . M\u00e9todos en NLTK \u00b6 Nltk.chunk: Utiliza expresiones regulares o \u00e1rboles de an\u00e1lisis para extraer frases. Nltk.tokenize: Divide el texto en frases, palabras o subpalabras, esencial para la estructura del texto. Nltk.corpus: Colecci\u00f3n de textos que se utiliza como muestra del lenguaje real. Nltk.sentiment: Analiza el sentimiento de un texto, como positivo o negativo. Nltk.translate: Funciones y algoritmos para trabajar con traducci\u00f3n autom\u00e1tica. Nltk.stem: Reduce las palabras a su ra\u00edz, eliminando sufijos y prefijos. Nltk.tag: Asigna partes del discurso a palabras como sustantivos, verbos, adjetivos, ... Nltk.inference: Herramienta para trabajar con l\u00f3gica e inferencia en textos. Tokenizaci\u00f3n con NLTK \u00b6 Importamos la librer\u00eda nltk y descargamos punkt que es el tokenizador de NLTK, el algoritmo que lleva a cabo la tokenizaci\u00f3n de las palabras que pases por par\u00e1metro. Divide el texto en frases a trav\u00e9s de un modelo no supervisado, esto es que no necesita datos etiquetados para su entrenamiento, obtendr\u00e1 patrones de los propios datos. import nltk nltk . download ( 'punkt' ) from nltk.corpus import stopwords from nltk.tokenize import sent_tokenize , word_tokenize Dado un texto, podemos ver los tokens con el siguiente c\u00f3digo: tokens = nltk . word_tokenize ( texto ) Descargamos las stopwords nltk . download ( 'stopwords' ) Seleccionamos \u00fanicamente las stopwords en Espa\u00f1ol stop_words = set ( stopwords . words ( 'spanish' )) Tokenizaci\u00f3n tokens = word_tokenize ( texto ) Sacamos \u00fanicamente las palabras que no est\u00e1n en las stop_words texto_filtrado = [ word for word in tokens if not word in stop_words ] \u00bfQu\u00e9 sucede si no tenemos alguna stopword en may\u00fasculas? No las reconoce como stopwords por lo tanto lo primero que tenemos que hacer es pasar todo el texto a min\u00fasculas. texto = texto . lower () Stemming con NLTK \u00b6 nltk . download ( 'wordnet' ) from nltk.stem import SnowballStemmer # Creamos el stemmer en espa\u00f1ol stemmer = SnowballStemmer ( 'spanish' ) Probamos con la palabra caminando print ( stemmer . stem ( 'caminando' )) Lematizaci\u00f3n con NLTK \u00b6 \u00danicamente tiene el diccionario en Ingl\u00e9s. from nltk.stem import WordNetLemmatizer nltk . download ( 'wordnet' ) Inicializamos el tokenizador lematizador = WordNetLemmatizer () print ( lematizador . lemmatize ( \"houses\" )) Spacy \u00b6 Es otra librer\u00eda de Python para PLN muy similar a NLTK y su implementaci\u00f3n suele ser m\u00e1s r\u00e1pida y precisa. M\u00e9todos en Spacy \u00b6 Spacy.load: Carga un modelo de lenguaje espec\u00edfico. Ejm: nlp=spacy.load('es_core_web_sm). nlp: Procesa un texto y ejecuta la cadena de procesamiento del modelo sobre \u00e9l. Doc, Span, Token: Proporciona contenedores para acceder a informaci\u00f3n sobre el texto como palabras individuales o grupos de palabras. Token.lemma_: Obtiene la forma base de una palabra. Doc.sents: Divide un documento en frases. Doc.ents: Extrae entidades nombradas como personas, organizaciones o lugares. Lematizaci\u00f3n en Spacy \u00b6 Instalamos Spacy ! pip install spacy - q Descargamos los paquetes que necesitamos para la lematizaci\u00f3n. ! python - m spacy download es_core_news_sm - q Cargar el modelo en espa\u00f1ol import spacy nlp = spacy . load ( 'es_core_news_sm' ) Probamos con texto (cualquier frase) doc = nlp ( 'texto' ) Imprimimos el texto y el lema de cada token for token in doc : print ( token . text , '->' , token . lemma_ ) Podemos mostrar cada token que etiqueta tiene for token in doc : print ( token . text , '->' , token . pos_ ) Actividad 2.1 \u00b6 Actividad 2.1 Crea un notebook dividido en dos partes: NLTK Realiza la tokenizaci\u00f3n del texto: \"El gato y el perro salen a jugar a la calle.\" Explica cada paso y visualiza los tokens finales. Comprobar el stemming de diferentes tiempos del verbo pasear. Explica el resultado. Haz la lematizaci\u00f3n de 5 palabras en ingl\u00e9s y explica el resultado. Spacy Realiza la lematizaci\u00f3n de un texto en castellano y explica los resultados. Aplica alguno de los m\u00e9todos de Spacy.","title":"2. Librer\u00edas para PLN"},{"location":"p1.html#librerias-para-pln","text":"","title":"**Librer\u00edas para PLN"},{"location":"p1.html#nltk-natural-language-toolkit","text":"Es una librer\u00eda de Python utilizada en el procesamiento del lenguaje natural. NLTK ofrece potentes capacidades de tokenizaci\u00f3n que facilitan el procesamiento eficiente de datos textuales. La tokenizaci\u00f3n de palabras de NLTK le permite dividir el texto en palabras individuales o tokens, adem\u00e1s contiene corpus de textos y vocabulario organizado como WordNet .","title":"NLTK: Natural Language Toolkit"},{"location":"p1.html#metodos-en-nltk","text":"Nltk.chunk: Utiliza expresiones regulares o \u00e1rboles de an\u00e1lisis para extraer frases. Nltk.tokenize: Divide el texto en frases, palabras o subpalabras, esencial para la estructura del texto. Nltk.corpus: Colecci\u00f3n de textos que se utiliza como muestra del lenguaje real. Nltk.sentiment: Analiza el sentimiento de un texto, como positivo o negativo. Nltk.translate: Funciones y algoritmos para trabajar con traducci\u00f3n autom\u00e1tica. Nltk.stem: Reduce las palabras a su ra\u00edz, eliminando sufijos y prefijos. Nltk.tag: Asigna partes del discurso a palabras como sustantivos, verbos, adjetivos, ... Nltk.inference: Herramienta para trabajar con l\u00f3gica e inferencia en textos.","title":"M\u00e9todos en NLTK"},{"location":"p1.html#tokenizacion-con-nltk","text":"Importamos la librer\u00eda nltk y descargamos punkt que es el tokenizador de NLTK, el algoritmo que lleva a cabo la tokenizaci\u00f3n de las palabras que pases por par\u00e1metro. Divide el texto en frases a trav\u00e9s de un modelo no supervisado, esto es que no necesita datos etiquetados para su entrenamiento, obtendr\u00e1 patrones de los propios datos. import nltk nltk . download ( 'punkt' ) from nltk.corpus import stopwords from nltk.tokenize import sent_tokenize , word_tokenize Dado un texto, podemos ver los tokens con el siguiente c\u00f3digo: tokens = nltk . word_tokenize ( texto ) Descargamos las stopwords nltk . download ( 'stopwords' ) Seleccionamos \u00fanicamente las stopwords en Espa\u00f1ol stop_words = set ( stopwords . words ( 'spanish' )) Tokenizaci\u00f3n tokens = word_tokenize ( texto ) Sacamos \u00fanicamente las palabras que no est\u00e1n en las stop_words texto_filtrado = [ word for word in tokens if not word in stop_words ] \u00bfQu\u00e9 sucede si no tenemos alguna stopword en may\u00fasculas? No las reconoce como stopwords por lo tanto lo primero que tenemos que hacer es pasar todo el texto a min\u00fasculas. texto = texto . lower ()","title":"Tokenizaci\u00f3n con NLTK"},{"location":"p1.html#stemming-con-nltk","text":"nltk . download ( 'wordnet' ) from nltk.stem import SnowballStemmer # Creamos el stemmer en espa\u00f1ol stemmer = SnowballStemmer ( 'spanish' ) Probamos con la palabra caminando print ( stemmer . stem ( 'caminando' ))","title":"Stemming con NLTK"},{"location":"p1.html#lematizacion-con-nltk","text":"\u00danicamente tiene el diccionario en Ingl\u00e9s. from nltk.stem import WordNetLemmatizer nltk . download ( 'wordnet' ) Inicializamos el tokenizador lematizador = WordNetLemmatizer () print ( lematizador . lemmatize ( \"houses\" ))","title":"Lematizaci\u00f3n con NLTK"},{"location":"p1.html#spacy","text":"Es otra librer\u00eda de Python para PLN muy similar a NLTK y su implementaci\u00f3n suele ser m\u00e1s r\u00e1pida y precisa.","title":"Spacy"},{"location":"p1.html#metodos-en-spacy","text":"Spacy.load: Carga un modelo de lenguaje espec\u00edfico. Ejm: nlp=spacy.load('es_core_web_sm). nlp: Procesa un texto y ejecuta la cadena de procesamiento del modelo sobre \u00e9l. Doc, Span, Token: Proporciona contenedores para acceder a informaci\u00f3n sobre el texto como palabras individuales o grupos de palabras. Token.lemma_: Obtiene la forma base de una palabra. Doc.sents: Divide un documento en frases. Doc.ents: Extrae entidades nombradas como personas, organizaciones o lugares.","title":"M\u00e9todos en Spacy"},{"location":"p1.html#lematizacion-en-spacy","text":"Instalamos Spacy ! pip install spacy - q Descargamos los paquetes que necesitamos para la lematizaci\u00f3n. ! python - m spacy download es_core_news_sm - q Cargar el modelo en espa\u00f1ol import spacy nlp = spacy . load ( 'es_core_news_sm' ) Probamos con texto (cualquier frase) doc = nlp ( 'texto' ) Imprimimos el texto y el lema de cada token for token in doc : print ( token . text , '->' , token . lemma_ ) Podemos mostrar cada token que etiqueta tiene for token in doc : print ( token . text , '->' , token . pos_ )","title":"Lematizaci\u00f3n en Spacy"},{"location":"p1.html#actividad-21","text":"Actividad 2.1 Crea un notebook dividido en dos partes: NLTK Realiza la tokenizaci\u00f3n del texto: \"El gato y el perro salen a jugar a la calle.\" Explica cada paso y visualiza los tokens finales. Comprobar el stemming de diferentes tiempos del verbo pasear. Explica el resultado. Haz la lematizaci\u00f3n de 5 palabras en ingl\u00e9s y explica el resultado. Spacy Realiza la lematizaci\u00f3n de un texto en castellano y explica los resultados. Aplica alguno de los m\u00e9todos de Spacy.","title":"Actividad 2.1"},{"location":"p4.html","text":"GENERACI\u00d3N DE TEXTO CON MODELOS DE MARKOV \u00b6 Podemos utilizar el modelo de Markov para generar textos, adem\u00e1s de para clasificar texto. El m\u00e9todo consiste en tener una bolsa de palabras y el modelo va a escoger de toda la bolsa de palabras, la que sea m\u00e1s probable que contin\u00fae en una frase dada. Contamos con la matriz de transici\u00f3n y el vector de probabilidades. El problema que tiene este modelo es que genera la palabra mir\u00e1ndo \u00fanicamente la palabra anterior. La ni\u00f1a est\u00e1 jugando con el gato [palabra] Si tenemos en cuenta la paabra anterior, es decir gato, seguramente nos devolver\u00e1 \"ma\u00falla\" por lo tanto el resultado de la frase ser\u00e1 \" La ni\u00f1a est\u00e1 jugando con el gato ma\u00falla\" que no tiene mucho sentido por lo tanto vamos a tener en cuenta no solo un estado (la palabra anterior) sino dos estados o incluso tres estados (las tres palabras anteriores). Modelo de Markov de segundo orden \u00b6 Es otra forma de almacenara y representar las probabilidades de transici\u00f3n. En vez de tener una matriz bidimensional, tendr\u00edamos una matriz tridimensional. Se tiene en cuenta no solo el estado anterior sino otro estado m\u00e1s. Tambi\u00e9n podemos aumentar el n\u00famero de estados (tercer orden) pero a medida que aumente el n\u00famero de estados anteriores, el tama\u00f1o de la matriz crece de forma exponencial y esto podr\u00eda llevar a problemas de eficiencia computacional ya que necesita muchos recursos para su implementaci\u00f3n y c\u00e1lculo. L \u00b6 Fases para la generaci\u00f3n de Dashboards \u00b6 Fase 1: Get Data \u00b6 Get Data = Obtener datos, realizar consultas (Query), importar datos, conexi\u00f3n a datos, etc. Los datos residen en diferentes bases de datos y archivos (Microsoft SQL Server, excel, csv, etc). El primer paso es detectar cuales son los or\u00edgenes de datos a los que se debe conectar Power BI. El asistente de la propia conexi\u00f3n seg\u00fan los par\u00e1metros necesarios para conectar, guiar\u00e1 al usuario mediante unos pasos muy sencillos y expl\u00edcitos. Este primer paso es importante ya que condicionar\u00e1 los tiempos de presentaci\u00f3n en los cuadros de mando, ya que la importaci\u00f3n de contenido irrelevante para las necesidades del proyecto, provocar\u00e1 un aumento en los tiempos de carga, que se arrastrar\u00e1 desde ese momento en adelante. Fase 2: Data Preparation \u00b6 Tras la obtenci\u00f3n de los datos desde los or\u00edgenes, es el momento de la preparaci\u00f3n del dato, para normalizarlo seg\u00fan las necesidades y por supuesto, para optimizar al m\u00e1ximo el dato importado. Este paso es importante adem\u00e1s de por el rendimiento en la posterior visualizaci\u00f3n, sino para garantizar un dato \"limpio\" y normalizado. Limpieza de datos. Verificaci\u00f3n de datos. Tipos de datos. Formateo de los datos num\u00e9ricos y fecha. Definir res\u00famenes en columnas num\u00e9ricas. Establecer nombres l\u00f3gicos para los campos. Nombres de campo \u00fanicos en todo el conjunto de datos. Cambiar el nombre de los pasos aplicados. Particionar campos. Existe un motor dise\u00f1ado para estas funciones que se llama Power Query y utiliza el lenguaje \"M\". Fase 3: Data Modeling \u00b6 En el proceso de modelado, es donde se relacionan las tablas de datos cargadas, de forma que se pueda acceder a datos de diferenes tablas, partiendo de un valor com\u00fan entre tablas. En esta fase es donde se realizan las medidas y columnas calculadas necesarias para complementar el contenido que se necesita, seg\u00fan los KPIs especificados. Un modelo de datos es la recopilaci\u00f3n de: Tablas de datos cargadas. Relaciones entre las tablas cargadas. Las medidas (f\u00f3rmulas) escritas que aplican las KPIs. Fase 4: Data Visualization \u00b6 Es la fase del desarrollo de los cuadros de mando, donde van a estar representados los KPIs definidos de una forma gr\u00e1fica o con contenido muy resumido de forma que no sea complicada su lectura, a fin de conseguir poder tener una capacidad alta en la toma de decisiones, a partir de los datos representados. Fase 5: Data Reporting \u00b6 Consiste en publicar el contenido que se acaba de dise\u00f1ar, con el fin de que el acceso a este no quede restringido a nuestro equipo, pudiendo permitir el acceso a diferentes usuarios dentro de la corporaci\u00f3n. Este proceso es muy importante a nivel de seguridad, ya que va a ser donde ser realicen las restricciones, tanto de acceso a los tableros, como de acceso a los datos y asignaci\u00f3n de roles.","title":"**GENERACI\u00d3N DE TEXTO CON MODELOS DE MARKOV**"},{"location":"p4.html#generacion-de-texto-con-modelos-de-markov","text":"Podemos utilizar el modelo de Markov para generar textos, adem\u00e1s de para clasificar texto. El m\u00e9todo consiste en tener una bolsa de palabras y el modelo va a escoger de toda la bolsa de palabras, la que sea m\u00e1s probable que contin\u00fae en una frase dada. Contamos con la matriz de transici\u00f3n y el vector de probabilidades. El problema que tiene este modelo es que genera la palabra mir\u00e1ndo \u00fanicamente la palabra anterior. La ni\u00f1a est\u00e1 jugando con el gato [palabra] Si tenemos en cuenta la paabra anterior, es decir gato, seguramente nos devolver\u00e1 \"ma\u00falla\" por lo tanto el resultado de la frase ser\u00e1 \" La ni\u00f1a est\u00e1 jugando con el gato ma\u00falla\" que no tiene mucho sentido por lo tanto vamos a tener en cuenta no solo un estado (la palabra anterior) sino dos estados o incluso tres estados (las tres palabras anteriores).","title":"GENERACI\u00d3N DE TEXTO CON MODELOS DE MARKOV"},{"location":"p4.html#modelo-de-markov-de-segundo-orden","text":"Es otra forma de almacenara y representar las probabilidades de transici\u00f3n. En vez de tener una matriz bidimensional, tendr\u00edamos una matriz tridimensional. Se tiene en cuenta no solo el estado anterior sino otro estado m\u00e1s. Tambi\u00e9n podemos aumentar el n\u00famero de estados (tercer orden) pero a medida que aumente el n\u00famero de estados anteriores, el tama\u00f1o de la matriz crece de forma exponencial y esto podr\u00eda llevar a problemas de eficiencia computacional ya que necesita muchos recursos para su implementaci\u00f3n y c\u00e1lculo.","title":"Modelo de Markov de segundo orden"},{"location":"p4.html#l","text":"","title":"L"},{"location":"p4.html#fases-para-la-generacion-de-dashboards","text":"","title":"Fases para la generaci\u00f3n de Dashboards"},{"location":"p4.html#fase-1-get-data","text":"Get Data = Obtener datos, realizar consultas (Query), importar datos, conexi\u00f3n a datos, etc. Los datos residen en diferentes bases de datos y archivos (Microsoft SQL Server, excel, csv, etc). El primer paso es detectar cuales son los or\u00edgenes de datos a los que se debe conectar Power BI. El asistente de la propia conexi\u00f3n seg\u00fan los par\u00e1metros necesarios para conectar, guiar\u00e1 al usuario mediante unos pasos muy sencillos y expl\u00edcitos. Este primer paso es importante ya que condicionar\u00e1 los tiempos de presentaci\u00f3n en los cuadros de mando, ya que la importaci\u00f3n de contenido irrelevante para las necesidades del proyecto, provocar\u00e1 un aumento en los tiempos de carga, que se arrastrar\u00e1 desde ese momento en adelante.","title":"Fase 1: Get Data"},{"location":"p4.html#fase-2-data-preparation","text":"Tras la obtenci\u00f3n de los datos desde los or\u00edgenes, es el momento de la preparaci\u00f3n del dato, para normalizarlo seg\u00fan las necesidades y por supuesto, para optimizar al m\u00e1ximo el dato importado. Este paso es importante adem\u00e1s de por el rendimiento en la posterior visualizaci\u00f3n, sino para garantizar un dato \"limpio\" y normalizado. Limpieza de datos. Verificaci\u00f3n de datos. Tipos de datos. Formateo de los datos num\u00e9ricos y fecha. Definir res\u00famenes en columnas num\u00e9ricas. Establecer nombres l\u00f3gicos para los campos. Nombres de campo \u00fanicos en todo el conjunto de datos. Cambiar el nombre de los pasos aplicados. Particionar campos. Existe un motor dise\u00f1ado para estas funciones que se llama Power Query y utiliza el lenguaje \"M\".","title":"Fase 2: Data Preparation"},{"location":"p4.html#fase-3-data-modeling","text":"En el proceso de modelado, es donde se relacionan las tablas de datos cargadas, de forma que se pueda acceder a datos de diferenes tablas, partiendo de un valor com\u00fan entre tablas. En esta fase es donde se realizan las medidas y columnas calculadas necesarias para complementar el contenido que se necesita, seg\u00fan los KPIs especificados. Un modelo de datos es la recopilaci\u00f3n de: Tablas de datos cargadas. Relaciones entre las tablas cargadas. Las medidas (f\u00f3rmulas) escritas que aplican las KPIs.","title":"Fase 3: Data Modeling"},{"location":"p4.html#fase-4-data-visualization","text":"Es la fase del desarrollo de los cuadros de mando, donde van a estar representados los KPIs definidos de una forma gr\u00e1fica o con contenido muy resumido de forma que no sea complicada su lectura, a fin de conseguir poder tener una capacidad alta en la toma de decisiones, a partir de los datos representados.","title":"Fase 4: Data Visualization"},{"location":"p4.html#fase-5-data-reporting","text":"Consiste en publicar el contenido que se acaba de dise\u00f1ar, con el fin de que el acceso a este no quede restringido a nuestro equipo, pudiendo permitir el acceso a diferentes usuarios dentro de la corporaci\u00f3n. Este proceso es muy importante a nivel de seguridad, ya que va a ser donde ser realicen las restricciones, tanto de acceso a los tableros, como de acceso a los datos y asignaci\u00f3n de roles.","title":"Fase 5: Data Reporting"},{"location":"p5.html","text":"APRENDIZAJE AUTOM\u00c1TICO \u00b6 Es una rama de la Inteligencia Artificial que se centra en la construcci\u00f3n de sistemas que pueden aprender de los datos, identifican patrones y toman decisiones. An\u00e1lisis de Sentimientos \u00b6 En el an\u00e1lisis de sentimientos se puede interpretar, analizar y categorizar las opiniones y emociones humanas expresadas en forma de texto. Categorizaci\u00f3n: positivo, negativo, neutral. Podemos considerar el an\u00e1lisis de sentimiento como una tarea de clasificaci\u00f3n etiquetando las entradas como positivas y negativas por ejemplo. Otra posibilidad es tratar el an\u00e1lisis de sentimiento como una tarea de regresi\u00f3n , es decir predecir un valor continuo en vez de una categor\u00eda. Aplicabilidad del An\u00e1lisis de Sentimientos \u00b6 Beneficios en t\u00e9rminos financieros. Gesti\u00f3n de reputaci\u00f3n. Monitoreo de redes sociales. An\u00e1lisis competitivo. Influencia de art\u00edculos y noticias en las acciones. etc. Fases para la generaci\u00f3n de Dashboards \u00b6 Fase 1: Get Data \u00b6","title":"**APRENDIZAJE AUTOM\u00c1TICO**"},{"location":"p5.html#aprendizaje-automatico","text":"Es una rama de la Inteligencia Artificial que se centra en la construcci\u00f3n de sistemas que pueden aprender de los datos, identifican patrones y toman decisiones.","title":"APRENDIZAJE AUTOM\u00c1TICO"},{"location":"p5.html#analisis-de-sentimientos","text":"En el an\u00e1lisis de sentimientos se puede interpretar, analizar y categorizar las opiniones y emociones humanas expresadas en forma de texto. Categorizaci\u00f3n: positivo, negativo, neutral. Podemos considerar el an\u00e1lisis de sentimiento como una tarea de clasificaci\u00f3n etiquetando las entradas como positivas y negativas por ejemplo. Otra posibilidad es tratar el an\u00e1lisis de sentimiento como una tarea de regresi\u00f3n , es decir predecir un valor continuo en vez de una categor\u00eda.","title":"An\u00e1lisis de Sentimientos"},{"location":"p5.html#aplicabilidad-del-analisis-de-sentimientos","text":"Beneficios en t\u00e9rminos financieros. Gesti\u00f3n de reputaci\u00f3n. Monitoreo de redes sociales. An\u00e1lisis competitivo. Influencia de art\u00edculos y noticias en las acciones. etc.","title":"Aplicabilidad del An\u00e1lisis de Sentimientos"},{"location":"p5.html#fases-para-la-generacion-de-dashboards","text":"","title":"Fases para la generaci\u00f3n de Dashboards"},{"location":"p5.html#fase-1-get-data","text":"","title":"Fase 1: Get Data"},{"location":"p6.html","text":"TRANSFORMERS \u00b6 Es un tipo de arquitectura basada en redes neuronales que procesan secuencias de palabras y tienen memoria a largo plazo, es decir que logran analizar secuencias muy extensas usando un mecanismo que se llama atenci\u00f3n . Adem\u00e1s, procesan toda la secuencia en paralelo (las redes recurrentes lo hacen en serie). Las redes transformers est\u00e1n descritas en el art\u00edculo del 2017 llamado \"Attention is All You Need\". PRE-ENTRENAMIENTO \u00b6 El pre-entrenamiento gnerativo proporciona al modelo un conjunto de datos formador por tokens y los entrena para predecir los tokens que contiene. Hay dos formas de pre-entrenamiento: Predicci\u00f3n de la siguiente palabra. El modelo de lenguaje enmascarado. Predicci\u00f3n de la siguiente palabra \u00b6 Es una t\u00e9cnica de aprendizaje supervisado que entrena el modelo con los datos de entrada y su correspondiente salida. Permite entrenar el modelo para predecir la siguiente palabra en una oraci\u00f3n dado el contexto de las palabras anteriores. El modelo aprende a generar texto coherente conociendo las dependencias entre palabras en un contexto m\u00e1s \u00e1mplio. Cuantos m\u00e1s ejemplos se ven, mejor se predice la siguiente palabra. El modelo de lenguaje enmascarado \u00b6 El modelo de lenguaje enmascarado entrena un modelo para predecir una palabra que est\u00e1 oculta en una oraci\u00f3n. Durante el entrenamiento, el modelo recibe como entrada tanto el texto original como el enmascarado. Introducci\u00f3n a los transformers \u00b6 Los transformers forman parte del pre-entrenamiento y potencian las t\u00e9cnicas que ya conocemos. \" Attention Is All You Need \" La arquitectura de los transformers da importancia a las relaciones entre palabras que no est\u00e1n cerca para generar un texto preciso y coherente. Los componentes de la arquitectura transformers es la siguiente: Preprocesamiento. Codificaci\u00f3n posicional (Posicional Encoding). Codificadores (Encoders). Decodificadores (Decoders).","title":"**TRANSFORMERS**"},{"location":"p6.html#transformers","text":"Es un tipo de arquitectura basada en redes neuronales que procesan secuencias de palabras y tienen memoria a largo plazo, es decir que logran analizar secuencias muy extensas usando un mecanismo que se llama atenci\u00f3n . Adem\u00e1s, procesan toda la secuencia en paralelo (las redes recurrentes lo hacen en serie). Las redes transformers est\u00e1n descritas en el art\u00edculo del 2017 llamado \"Attention is All You Need\".","title":"TRANSFORMERS"},{"location":"p6.html#pre-entrenamiento","text":"El pre-entrenamiento gnerativo proporciona al modelo un conjunto de datos formador por tokens y los entrena para predecir los tokens que contiene. Hay dos formas de pre-entrenamiento: Predicci\u00f3n de la siguiente palabra. El modelo de lenguaje enmascarado.","title":"PRE-ENTRENAMIENTO"},{"location":"p6.html#prediccion-de-la-siguiente-palabra","text":"Es una t\u00e9cnica de aprendizaje supervisado que entrena el modelo con los datos de entrada y su correspondiente salida. Permite entrenar el modelo para predecir la siguiente palabra en una oraci\u00f3n dado el contexto de las palabras anteriores. El modelo aprende a generar texto coherente conociendo las dependencias entre palabras en un contexto m\u00e1s \u00e1mplio. Cuantos m\u00e1s ejemplos se ven, mejor se predice la siguiente palabra.","title":"Predicci\u00f3n de la siguiente palabra"},{"location":"p6.html#el-modelo-de-lenguaje-enmascarado","text":"El modelo de lenguaje enmascarado entrena un modelo para predecir una palabra que est\u00e1 oculta en una oraci\u00f3n. Durante el entrenamiento, el modelo recibe como entrada tanto el texto original como el enmascarado.","title":"El modelo de lenguaje enmascarado"},{"location":"p6.html#introduccion-a-los-transformers","text":"Los transformers forman parte del pre-entrenamiento y potencian las t\u00e9cnicas que ya conocemos. \" Attention Is All You Need \" La arquitectura de los transformers da importancia a las relaciones entre palabras que no est\u00e1n cerca para generar un texto preciso y coherente. Los componentes de la arquitectura transformers es la siguiente: Preprocesamiento. Codificaci\u00f3n posicional (Posicional Encoding). Codificadores (Encoders). Decodificadores (Decoders).","title":"Introducci\u00f3n a los transformers"}]}